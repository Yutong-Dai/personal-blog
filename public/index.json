[{"authors":["admin"],"categories":null,"content":"Yutong is a Ph.D. student in the Department of Industrial and Systems Engineering at Lehigh University, working under the supervision of Professor Daniel P. Robinson.\nHis current research focus on designing and analyzing algorithms for large scale convex non-smooth optimization problems arisen in machine learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1596081872,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"Yutong is a Ph.D. student in the Department of Industrial and Systems Engineering at Lehigh University, working under the supervision of Professor Daniel P. Robinson.\nHis current research focus on designing and analyzing algorithms for large scale convex non-smooth optimization problems arisen in machine learning.","tags":null,"title":"","type":"authors"},{"authors":null,"categories":null,"content":"In this project, I will try to collect ideas and then make some notes when learning C++ for writing optimization software.\nMost materials are from Frank E. Curtis and the Internet. I will cite resources whenever possible.\n","date":1597104000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1598900989,"objectID":"c567036e3ede20c892f1f3a6d8f8540a","permalink":"/resources/cpp_laopt/","publishdate":"2020-08-11T00:00:00Z","relpermalink":"/resources/cpp_laopt/","section":"resources","summary":"Write optimization algorithms from scratch in C++.","tags":null,"title":"Description","type":"docs"},{"authors":null,"categories":null,"content":"","date":1597104000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1598900989,"objectID":"47b7058404de98b2b76688e87a07799a","permalink":"/resources/cpp_laopt/cpp/","publishdate":"2020-08-11T00:00:00Z","relpermalink":"/resources/cpp_laopt/cpp/","section":"resources","summary":"","tags":null,"title":"Elements of C++","type":"docs"},{"authors":null,"categories":null,"content":"Demo   Install You can obtain all source code from my github repo.\nUsage Put standalone.tex and note.cls in the same folder and complie.\nMinimal Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  % in your .tex file \\documentclass[doctype=s]{note} \\title{Put your note title here.} \\author{Yutong Dai} \\date{Last Update: \\today} \\usepackage{lipsum} \\usepackage[margin=1.5cm, portrait]{geometry} \\begin{document} % \\maketitle \\pre{Questions} { \\begin{enumerate} \\item{What\u0026#39;s the first question this section will answer?} \\item{What\u0026#39;s the second question?} \\end{enumerate} } \\note[key term 1] { \\begin{itemize} \\item{A cool thing} \\item{Another cool thing} \\item{Not all things are cool} \\end{itemize} } \\note[key term 2] { \\lipsum[4] } \\note[Very very very very very very very very long head] { \\lipsum[5] } \\summary{Summary}{A fabulous summary} \\end{document}   Key components   \\title{}: the title for your document. If you want to leave it blank, please use \\title{}; otherwise use \\title{your title}.\n  \\author{}: the author for your document. If you want to leave it blank, please use \\author{}; otherwise use \\author{your name}.\n  \\date{}: the last modified date for your document. If you want to leave it blank, please use \\date{}; otherwise use \\date{Last Update: \\today}.\n  \\pre{}{}: a pre read block. The first argument is the title for this block and the second argument fills in the block.\n  \\note{}{} a note bolck. The first argument records the keyword and the second argument fills the note content.\n  \\summary{}{}: a summary block. see \\pre{}{}.\n  Modes Two modes are provided for this template,\n  standalone mode[default]: renders the file as a stand-alone file, the one showed in demo. In this mode, when providing title/author/date, please do not use \\maketitle command. These information will be automatically rendered.\n  collection mode: Use this note as a widget, which can be direcrly incorporated in your article. Since note class directly inherit from the article class. To use collection mode just pass doctype=c to the \\documnetclass as the demo code indicates.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  % in your .tex file  \\documentclass[doctype=c]{note} \\title{Put your note title here.} \\author{Yutong Dai} \\date{Last Update: \\today} \\usepackage{lipsum} \\usepackage[margin=1.5cm, portrait]{geometry} \\begin{document} \\maketitle \\tableofcontents \\newpage \\section{First section} \\pre{Questions} { \\begin{enumerate} \\item{What\u0026#39;s the first question this section will answer?} \\item{What\u0026#39;s the second question?} \\end{enumerate} } \\note[key term 1] { \\begin{itemize} \\item{A cool thing} \\item{Another cool thing} \\item{Not all things are cool} \\end{itemize} } \\note[key term 2] { \\lipsum[4] } \\note[Very very very very very very very very long head] { \\lipsum[5] } \\summary{Summary}{A fabulous summary}     Change the layout The default layout is portrait, if you would like to switch to landscape, just use \\usepackage[landscape]{geometry}.\n","date":1596672000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1596741242,"objectID":"15ae87a17a0711f27b24a2181f6f2aa6","permalink":"/resources/tex-rmd/cornellnotes/","publishdate":"2020-08-06T00:00:00Z","relpermalink":"/resources/tex-rmd/cornellnotes/","section":"resources","summary":"Demo   Install You can obtain all source code from my github repo.\nUsage Put standalone.tex and note.cls in the same folder and complie.\nMinimal Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  % in your .","tags":null,"title":"Cornell Notes Template","type":"docs"},{"authors":null,"categories":null,"content":"This page collects $\\LaTeX$ and Rmarkdown templates that I adapted from the Internet for a variety purpose, which includes but not limited to taking lecture notes, writing homework and making cheatsheet.\n","date":1559260800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1596741242,"objectID":"395e2dc8a7bcb516f8e25f58c11ce4cc","permalink":"/resources/tex-rmd/","publishdate":"2019-05-31T00:00:00Z","relpermalink":"/resources/tex-rmd/","section":"resources","summary":"Templates for lecture notes, homework and cheatsheet and more.","tags":null,"title":"Description","type":"docs"},{"authors":null,"categories":null,"content":"","date":1559257200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1596741242,"objectID":"044a932d5db50d3527ec08f5493765e8","permalink":"/resources/tex-rmd/cheatsheet/","publishdate":"2019-05-31T00:00:00+01:00","relpermalink":"/resources/tex-rmd/cheatsheet/","section":"resources","summary":"","tags":null,"title":"Cheatsheet Template","type":"docs"},{"authors":null,"categories":null,"content":"","date":1559257200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1596741242,"objectID":"2eb6399da15e15c42f14efd1d6630dc3","permalink":"/resources/tex-rmd/homework/","publishdate":"2019-05-31T00:00:00+01:00","relpermalink":"/resources/tex-rmd/homework/","section":"resources","summary":"","tags":null,"title":"Homework Template","type":"docs"},{"authors":null,"categories":null,"content":"","date":1559257200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1596741242,"objectID":"d0df433092b6fbcfd8ff37bc345bb505","permalink":"/resources/tex-rmd/lecturenotes/","publishdate":"2019-05-31T00:00:00+01:00","relpermalink":"/resources/tex-rmd/lecturenotes/","section":"resources","summary":"","tags":null,"title":"Lecture Notes Template","type":"docs"},{"authors":null,"categories":null,"content":"并查集的数据结构  参考.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  class UF: def __init__(self, n:int): \u0026#34;\u0026#34;\u0026#34; n: 图中有多少个node \u0026#34;\u0026#34;\u0026#34; # 初始化: 每个node是自己的根节点 self.parent = [i for i in range(n)] # 初始化: 以自己为根节点的tree的size都是1 self.size = [1] * n # 初始化: 连通图的个数 self.count = n def find(self, node:int): while node != self.parent[node]: # 路径压缩 self.parent[node] = self.parent[self.parent[node]] node = self.parent[node] return node def union(self, p:int, q:int): pRoot = self.find(p) qRoot = self.find(q) if pRoot != qRoot: # 小树挂在大树上 if self.size[pRoot] \u0026gt;= self.size[qRoot]: self.parent[qRoot] = pRoot self.size[pRoot] += self.size[qRoot] self.size[qRoot] = None else: self.parent[pRoot] = qRoot self.size[qRoot] += self.size[pRoot] self.size[pRoot] = None self.count -= 1 def connected(self, p:int, q:int): return self.find(p) == self.find(q)   应用:\n 确定无向图的连通分量  题目 班上有 N 名学生。其中有些人是朋友，有些则不是。他们的友谊具有是传递性。如果已知 A 是 B 的朋友，B 是 C 的朋友，那么我们可以认为 A 也是 C 的朋友。所谓的朋友圈，是指所有朋友的集合。 给定一个 N * N 的矩阵 M，表示班级中学生之间的朋友关系。如果M[i][j] = 1，表示已知第 i 个和 j 个学生互为朋友关系，否则为不知道。你必须输出所有学生中的已知的朋友圈总数。 示例 1： 输入： [[1,1,0], [1,1,0], [0,0,1]] 输出：2 解释：已知学生 0 和学生 1 互为朋友，他们在一个朋友圈。 第2个学生自己在一个朋友圈。所以返回 2 。 示例 2： 输入： [[1,1,0], [1,1,1], [0,1,1]] 输出：1 解释：已知学生 0 和学生 1 互为朋友，学生 1 和学生 2 互为朋友，所以学生 0 和学生 2 也是朋友，所以他们三个在一个朋友圈，返回 1 。 提示： 1 \u0026lt;= N \u0026lt;= 200 M[i][i] == 1 M[i][j] == M[j][i] 来源：力扣（LeetCode） 链接：https://leetcode-cn.com/problems/friend-circles 著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。 思路一： 并查集 消化模板中\n代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43  class UF: def __init__(self, n:int): \u0026#34;\u0026#34;\u0026#34; n: 图中有多少个node \u0026#34;\u0026#34;\u0026#34; # 初始化: 每个node是自己的根节点 self.parent = [i for i in range(n)] # 初始化: 以自己为根节点的tree的size都是1 self.size = [1] * n # 初始化: 连通图的个数 self.count = n def find(self, node:int): while node != self.parent[node]: # 路径压缩 self.parent[node] = self.parent[self.parent[node]] node = self.parent[node] return node def union(self, p:int, q:int): pRoot = self.find(p) qRoot = self.find(q) if pRoot != qRoot: # 小树挂在大树上 if self.size[pRoot] \u0026gt;= self.size[qRoot]: self.parent[qRoot] = pRoot self.size[pRoot] += self.size[qRoot] self.size[qRoot] = None else: self.parent[pRoot] = qRoot self.size[qRoot] += self.size[pRoot] self.size[pRoot] = None self.count -= 1 class Solution: def findCircleNum(self, M: List[List[int]]) -\u0026gt; int: n = len(M) uf = UF(n) if n == 0: return 0 for i in range(n): for j in range(i+1,n): if M[i][j] == 1: uf.union(i, j) return uf.count   复杂度:  时间：$O(n^2)$ 空间：$O(n)$  思路二: 无向图的搜索 BFS 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  M = [[1, 0, 0, 0, 0, 1], [0, 1, 1, 0, 1, 0], [0, 1, 1, 1, 0, 0], [0, 0, 1, 1, 0, 0], [0, 1, 0, 0, 1, 0], [1, 0, 0, 0 ,0, 1]] class Solution: def findCircleNum(self, M): num_of_students = len(M) visited = [0] * num_of_students count = 0 queue = [] for i in range(num_of_students): print(\u0026#34;Explore node {}\u0026#34;.format(i)) if visited[i]: print(\u0026#34;node {} is visited\u0026#34;.format(i)) continue else: print(\u0026#34;node {} is unexplored\u0026#34;.format(i)) queue.append(i) print(\u0026#34;add node {} to queue | queue:{}\u0026#34;.format(i, queue)) visited[i] = 1 print(\u0026#34;mark node {} as visited\u0026#34;.format(i)) while queue: next_node_index = queue.pop(0) print(\u0026#34;node to be explored:{}\u0026#34;.format(next_node_index)) for j in range(i, num_of_students): if M[next_node_index][j] == 1 and visited[j] != 1: print(\u0026#34;node:{} and node:{} are connected!\u0026#34;.format(next_node_index, j)) queue.append(j) print(\u0026#34;add node {} to queue | queue:{}\u0026#34;.format(j, queue)) visited[j] = 1 print(\u0026#34;mark node {} as visited\u0026#34;.format(j)) count += 1 print(\u0026#34;====\u0026#34;) return count s = Solution() s.findCircleNum(M)   ","date":1607904000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1607904000,"objectID":"cbd251955731c930ad5348989e6daeae","permalink":"/resources/dsal/union-find/","publishdate":"2020-12-14T00:00:00Z","relpermalink":"/resources/dsal/union-find/","section":"resources","summary":"并查集的数据结构 参考. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19","tags":null,"title":"Union Find 并查集","type":"docs"},{"authors":null,"categories":null,"content":"前缀树的数据结构  参考.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  class Node: def __init__(self, val: str, isWord=False) -\u0026gt; None: self.val = val self.children = {} self.isWord = isWord class Trie: def __init__(self): \u0026#34;\u0026#34;\u0026#34; Initialize your data structure here. \u0026#34;\u0026#34;\u0026#34; self.root = Node(val=None) def insert(self, word: str) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; Inserts a word into the trie. \u0026#34;\u0026#34;\u0026#34; node = self.root for char in word: if char not in node.children: node.children[char] = Node(val=char) node = node.children[char] node.isWord = True def search(self, word: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34; Returns if the word is in the trie. \u0026#34;\u0026#34;\u0026#34; node = self.root for char in word: if char not in node.children: return False node = node.children[char] if node.isWord: return True return False def startsWith(self, prefix: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34; Returns if there is any word in the trie that starts with the given prefix. \u0026#34;\u0026#34;\u0026#34; node = self.root for char in prefix: if char not in node.children: return False node = node.children[char] return True    时间复杂度：$O(n)$，n 是字符串长度， insert search startsWith 操作都是。 空间复杂度：$O(26^{h})$，h 是插入所有字符串最长的长度。  677. 键值映射 题目描述 实现一个 MapSum 类，支持两个方法，insert 和 sum： MapSum() 初始化 MapSum 对象 void insert(String key, int val) 插入 key-val 键值对，字符串表示键 key ，整数表示值 val 。如果键 key 已经存在，那么原来的键值对将被替代成新的键值对。 int sum(string prefix) 返回所有以该前缀 prefix 开头的键 key 的值的总和。 示例： 输入： [\u0026quot;MapSum\u0026quot;, \u0026quot;insert\u0026quot;, \u0026quot;sum\u0026quot;, \u0026quot;insert\u0026quot;, \u0026quot;sum\u0026quot;] [[], [\u0026quot;apple\u0026quot;, 3], [\u0026quot;ap\u0026quot;], [\u0026quot;app\u0026quot;, 2], [\u0026quot;ap\u0026quot;]] 输出： [null, null, 3, null, 5] 解释： MapSum mapSum = new MapSum(); mapSum.insert(\u0026quot;apple\u0026quot;, 3); mapSum.sum(\u0026quot;ap\u0026quot;); // return 3 (apple = 3) mapSum.insert(\u0026quot;app\u0026quot;, 2); mapSum.sum(\u0026quot;ap\u0026quot;); // return 5 (apple + app = 3 + 2 = 5) 提示： 1 \u0026lt;= key.length, prefix.length \u0026lt;= 50 key 和 prefix 仅由小写英文字母组成 1 \u0026lt;= val \u0026lt;= 1000 最多调用 50 次 insert 和 sum 来源：力扣（LeetCode） 链接：https://leetcode-cn.com/problems/map-sum-pairs 著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。 思路   题干中要求返回以该前缀 prefix 开头的键 key 的值的总和 \u0026mdash; 前缀树，空间换时间。使用前缀树模版。\n  insert(key, val): 将key中的每一个字符作为一个多叉树的node, 依次将node放入树中。最后需要在leaf node上记录 val， 并将其标记为一个单词的结尾。如果插入的重复的key, 只需跟新leaf node的值就好。\n  sum(prefix): 沿着多叉树找到以prefix最后一个字符结尾的node(如果存在)。利用层次遍历其所有的leaf node，然后将上面的数字求和便可得到答案。\n  代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38  class Node: def __init__(self, key=None, isWord=False, val=None): self.key = key self.children = {} self.isWord = isWord self.val = None class MapSum: def __init__(self): \u0026#34;\u0026#34;\u0026#34; Initialize your data structure here. \u0026#34;\u0026#34;\u0026#34; self.root = Node() def insert(self, key: str, val: int) -\u0026gt; None: node = self.root for char in key: if char not in node.children: node.children[char] = Node(key=char) node = node.children[char] # the last node stores the last char in key, hence is a word node.isWord = True node.val = val def sum(self, prefix: str) -\u0026gt; int: node = self.root ans = 0 for char in prefix: if char not in node.children: return ans node = node.children[char] # the node stores the ending char in the prefix queue = [node] while queue: node = queue.pop(0) if node.isWord: ans += node.val for child in node.children: queue.append(node.children[child]) return ans   复杂度  时间 insert: $O(n)$, n \u0026mdash;\u0026gt; key这个字符串的长度 sum: $O(26^h)$, h\u0026mdash;\u0026gt; h为多叉树的高度。最差的情况prefix在root，树是一个满多叉树。 空间： $O(26^h)$  题目: 面试题 17.17. 多次搜索 给定一个较长字符串big和一个包含较短字符串的数组smalls，设计一个方法，根据smalls中的每一个较短字符串，对big进行搜索。输出smalls中的字符串在big里出现的所有位置positions，其中positions[i]为smalls[i]出现的所有位置。 示例： 输入： big = \u0026quot;mississippi\u0026quot; smalls = [\u0026quot;is\u0026quot;,\u0026quot;ppi\u0026quot;,\u0026quot;hi\u0026quot;,\u0026quot;sis\u0026quot;,\u0026quot;i\u0026quot;,\u0026quot;ssippi\u0026quot;] 输出： [[1,4],[8],[],[3],[1,4,7,10],[5]] 提示： 0 \u0026lt;= len(big) \u0026lt;= 1000 0 \u0026lt;= len(smalls[i]) \u0026lt;= 1000 smalls的总字符数不会超过 100000。 你可以认为smalls中没有重复字符串。 所有出现的字符均为英文小写字母。 来源：力扣（LeetCode） 链接：https://leetcode-cn.com/problems/multi-search-lcci 著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。 思路 1： 暴力搜索（超时） 对每个来自smalls中的wordsmall 遍历一次big去匹配。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  class Solution: def multiSearch(self, big: str, smalls: List[str]) -\u0026gt; List[List[int]]: nBig = len(big) ans = [] for idx, small in enumerate(smalls): ans.append([]) if small == \u0026#34;\u0026#34;: continue nSmall = len(small) idx_small = 0 prefix = small[idx_small] # find the occurence of the prefix of the small in big # print(\u0026#34;search for:\u0026#39;{}\u0026#39; | prefix:{}\u0026#34;.format(small, prefix)) for i in range(nBig): # print(\u0026#34;Seach the {}-th char in big to match prefix.\u0026#34;.format(i)) if big[i] == prefix: # print(\u0026#34; prefix matched!\u0026#34;) for j in range(i, i+nSmall): if j \u0026lt; nBig: if small[idx_small] == big[j]: idx_small += 1 else: break # match if idx_small == nSmall: # print(\u0026#34; matched\u0026#34;) ans[-1].append(i) # reset index to loop over small idx_small = 0 return ans     时间复杂度: $O(n_{big} * n_{smalls} * s_{max})$: 其中 $n_{big}$, $n_{smalls}$, $s_{max}$分别是 字符串big的长度，smalls 单词的个数，以及smalls中单词最大的长度。\n  空间复杂度：$O(1)$ (不算返回的数组)\n  思路 2：前缀树   用什么建树：第一反应是用 big的所有子字符串去建树，有如下问题\n 空间复杂度和最大长度有关，如果big很长，就很糟糕。 找所有子列是$O(2^{n_{big}})$的复杂度 所以用smalls中的字符串去建树。    怎么设计前缀树\n 除了套用模板记录 isWord 之外，我们需要用一个 属性idx 来表明当前这个字符(如果代表一个word)在smalls中的位置。    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  class Node: def __init__(self, idx=None): self.children = {} self.isWord = False self.idx = idx class Solution: def insert(self, word, idx): node = self.root for char in word: if char not in node.children: node.children[char] = Node() node = node.children[char] # reach the end of the word node.idx = idx node.isWord = True def multiSearch(self, big: str, smalls: List[str]) -\u0026gt; List[List[int]]: n = len(smalls) self.root = Node() ans = [] # build the trie for i in range(n): self.insert(smalls[i], i) ans.append([]) # begin search nBig = len(big) for i in range(nBig): node = self.root for j in range(i, nBig): current_char = big[j] # no match if current_char not in node.children: break if node.children[current_char].isWord: ans[node.children[current_char].idx].append(i) node = node.children[current_char] return ans    时间复杂度:  建树: $O(n_{small} * s_{max})$ 搜索: $O(s_{max} * n_{big})$ 显著的比暴力的 $O(n_{big} * n_{smalls} * s_{max})$ 节约时间   空间复杂度：  建树：$O(n_{small} * s_{max})$    ","date":1607731200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1607731200,"objectID":"a22f3329d5734d0323a4befe77dd4d3c","permalink":"/resources/dsal/trie/","publishdate":"2020-12-12T00:00:00Z","relpermalink":"/resources/dsal/trie/","section":"resources","summary":"前缀树的数据结构 参考. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19","tags":null,"title":"Trie 前缀树","type":"docs"},{"authors":null,"categories":null,"content":"560. 和为k的子数组 给定一个整数数组和一个整数 k，你需要找到该数组中和为 k 的连续的子数组的个数。 示例 1 : 输入:nums = [1,1,1], k = 2 输出: 2 , [1,1] 与 [1,1] 为两种不同的情况。 说明 : 数组的长度为 [1, 20,000]。 数组中元素的范围是 [-1000, 1000] ，且整数 k 的范围是 [-1e7, 1e7]。 来源：力扣（LeetCode） 链接：https://leetcode-cn.com/problems/subarray-sum-equals-k 著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。 思路 最直观的想法便是给定了 start与end ，对子数组num[start:end]里的元素求和。如果和为k则说明找到一组符合条件的子数组。但是这样的复杂度为$O(n^3)$。\n初次优化 观察到\n1  sum(num[start-1:end]) + num[start] = sum(num[start:end])   我们可以固定住end 然后让 start从 end开始递减到0，对在这个循环过程中得到子列和与k比较并统计符合条件的子列数量。\n这样的话时间复杂度为$O(n^2)$. 可惜Python的版本会超时。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  class Solution(object): def subarraySum(self, nums, k): \u0026#34;\u0026#34;\u0026#34; :type nums: List[int] :type k: int :rtype: int \u0026#34;\u0026#34;\u0026#34; total = 0 if not nums: return total n = len(nums) for end in range(n): tempSum = 0 for start in range(end, -1, -1): tempSum += nums[start] if tempSum == k: total += 1 return total   再次优化 假设我们有个数列cumSum其中cumsum[i]是num中前i个数的和，比如\ncumSum[0] = num[0] cumSum[1] = num[0] + num[1] .... 更加紧凑的我们有 cumSum[0] = num[0] cumSum[i] = num[i] + cumSum[i-1], i \u0026gt;= 1 对于满足的条件的子列(从start开始，到end结束 )，那么我们有\nk = sum(num[start:end]) = cumSum[end] - cumSum[start-1] ---\u0026gt; cumSum[start-1] = cumSum[end] - k 本质就是寻找 从0 到 end 有多少个 index i 使得 cumSum[i]  为k。 之前的方法是用循环，这里考虑用哈希表dic。在建立cumSum这个数组的时候，我们 可以以 cumSum[i] 为key 以 cumSum[i] 出现的频次为value。 那么对于以end结束的子列，满足条件的子列有dic[cumSum[end] - k ]。\n最后两个小细节：\n 边界情况。num[end] 直接为k。我们在初始化哈希表时，需要加入一个 以0为key 以 1为value的key-value pair。 我们不需要维护一个数组cumSum[i]  而是用一个变量就够了。  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  from collections import defaultdict class Solution(object): def subarraySum(self, nums, k): \u0026#34;\u0026#34;\u0026#34; :type nums: List[int] :type k: int :rtype: int \u0026#34;\u0026#34;\u0026#34; if not nums: return 0 total = 0 n = len(nums) cumSum = 0 cumSumFreq = defaultdict() # 边界条件：什么都不选的时候 cumulative sum 为 0 cumSumFreq[0] = 1 for start in range(n): cumSum += nums[start] total += cumSumFreq.get(cumSum - k, 0) # 更新字典必须在求和之后 cumSumFreq[cumSum] = cumSumFreq.get(cumSum, 0) + 1 return total   时间复杂度： $O(n)$ 空间复杂度： $O(n)$\n","date":1607472000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1607472000,"objectID":"b36af9ea3904b55bd6e8c0dee242d78b","permalink":"/resources/dsal/prefix-sum/","publishdate":"2020-12-09T00:00:00Z","relpermalink":"/resources/dsal/prefix-sum/","section":"resources","summary":"560. 和为k的子数组 给定一个整数数组和一个整数 k，你需要找到该数","tags":null,"title":"Prefix Sum","type":"docs"},{"authors":null,"categories":null,"content":"leetcode 78: 子集 题目描述\n给定一组不含重复元素的整数数组 nums，返回该数组所有可能的子集（幂集）。 说明：解集不能包含重复的子集。 示例: 输入: nums = [1,2,3] 输出: [ [3], [1], [2], [1,2,3], [1,3], [2,3], [1,2], [] ] 来源：力扣（LeetCode） 链接：https://leetcode-cn.com/problems/subsets 著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。 思路 数学上来说，每个元素存在选和不选两个可能性，所以一共有2^n种结果，其中n是nums 元素中的个数。 问题变成了枚举[0, 2^n-1]的二进制表达。举个例子\nnums = [3,2,1] ---\u0026gt; 8 = 2^3 中可能 每种可能对应的二进制表示如下 1. 000 [] 2. 001 [1] 3. 010 [2] 4. 011 [1,2] 5. 100 [3] 6. 101 [1,3] 7. 110 [1,2] 8. 111 [1,2,3] 但是如何用计算机语言来，尤其是利用位运算来枚举上述所有情况呢。因为对这个不熟，参考了 这里的处理方式。\n举个例子\n110 从右往左(0-indexed)在第1和第2位置出现了1. 如何用位运算找到呢？ 每次向右移动n个bit然后和 1 做 “并” 运算，便可知道第n位是否为1。 举个例子 将110向右移动0个bit: 110\u0026gt;\u0026gt;0 ---\u0026gt; 110. 110 \u0026amp; 001 ---\u0026gt; 0: 说明第0位不为1 将110向右移动1个bit: 110\u0026gt;\u0026gt;1 ---\u0026gt; 011. 011 \u0026amp; 001 ---\u0026gt; 1: 说明第1位为1 将110向右移动2个bit: 110\u0026gt;\u0026gt;2 ---\u0026gt; 001. 001 \u0026amp; 001 ---\u0026gt; 1: 说明第2位为1 代码 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  class Solution(object): def subsets(self, nums): \u0026#34;\u0026#34;\u0026#34; :type nums: List[int] :rtype: List[List[int]] \u0026#34;\u0026#34;\u0026#34; n = len(nums) numElements = 1 \u0026lt;\u0026lt; n ans = [0] * numElements for mask in range(numElements): # 寻找 mask 的二进制表达中 1出现的位置 # 因为我们一共有n个数字在nums中， 所以2^n # 有n个bits。 从而最左位数最多能向右移动n-1次 temp = [] for rightShift in range(n): # 从右往左数，mask的第rightShift位刚好是1 if (mask \u0026gt;\u0026gt; rightShift) \u0026amp; 1 == 1: temp.append(nums[rightShift]) ans[mask] = temp return ans   复杂度  时间：$O(n2^n)$  for mask in range(numElements): $2^n$  for rightShift in range(n):: $n$   空间: 不算返回数组是 $O(n)$  ","date":1607126400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1607126400,"objectID":"8961a33d894ed7ce70cd6c8044f503ab","permalink":"/resources/dsal/bit-operations/","publishdate":"2020-12-05T00:00:00Z","relpermalink":"/resources/dsal/bit-operations/","section":"resources","summary":"leetcode 78: 子集 题目描述 给定一组不含重复元素的整数数组 nums，返回","tags":null,"title":"Bit operations","type":"docs"},{"authors":null,"categories":null,"content":"","date":1607126400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1607126400,"objectID":"a55ba8aa95c13fa4f0157c7a4c296296","permalink":"/resources/dsal/tree/","publishdate":"2020-12-05T00:00:00Z","relpermalink":"/resources/dsal/tree/","section":"resources","summary":"","tags":null,"title":"Tree and Graph","type":"docs"},{"authors":null,"categories":null,"content":"Compile your code in command line  Reference: Pitt-Francis, J., \u0026amp; Whiteley, J. (2017). Guide to Scientific Computing in C++ Secon Edition. Springer.\n compiler [-flag1 -flag2 ...] -o excutableFileName sourceCodeFile   Compiler: g++\n  Compiler flags:\n -Wall: list out anything unexpected that is not actually an error, but will still create an executable file. -O: (upper case o): optimize the executable file at the cost of longer compilation time -g: compile code with debugging information preserved -o: use this to allow name the excutable file name    Inputs:\n excutableFileName: the parameter provided to the flag -o sourceCodeFile: the cpp file which you want to compile    1  g++ -Wall -O -o addTwoNumbers addTwoNumbers.cpp   ","date":1597536000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598900989,"objectID":"00ec439825509f777e40c20da9ff8ce2","permalink":"/resources/cpp_laopt/cpp/makefile/","publishdate":"2020-08-16T00:00:00Z","relpermalink":"/resources/cpp_laopt/cpp/makefile/","section":"resources","summary":"Compile your code in command line  Reference: Pitt-Francis, J., \u0026amp; Whiteley, J. (2017). Guide to Scientific Computing in C++ Secon Edition. Springer.\n compiler [-flag1 -flag2 ...] -o excutableFileName sourceCodeFile   Compiler: g++","tags":null,"title":"Compile your program","type":"docs"},{"authors":null,"categories":null,"content":"Traverse Algorithms Pre-order The pre-order traversal visit nodes in root - left - right order, where the the root appears in the first.\nrecursion implementation 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  # Definition for a binary tree node. # class TreeNode(object): # def __init__(self, val=0, left=None, right=None): # self.val = val # self.left = left # self.right = right class Solution(object): def preorderTraversal(self, root): \u0026#34;\u0026#34;\u0026#34; :type root: TreeNode :rtype: List[int] \u0026#34;\u0026#34;\u0026#34; self.ans = [] self.dfs(root) return self.ans def dfs(self, node): if not node: return self.ans.append(node.val) if node.left: self.dfs(node.left) if node.right: self.dfs(node.right)   iterative implementation 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  # iterative # hint: using stack and add children using right-left order. from collections import deque class Solution(object): def preorderTraversal(self, root): \u0026#34;\u0026#34;\u0026#34; :type root: TreeNode :rtype: List[int] \u0026#34;\u0026#34;\u0026#34; if not root: return [] stack = deque() stack.appendleft(root) ans = [] while stack: node = stack.popleft() ans.append(node.val) if node.right: stack.appendleft(node.right) if node.left: stack.appendleft(node.left) return ans   In-order The in-order traversal visit nodes in left-root-right order, where the the root appears in the middle. recursion implementation\nRecursion 1 2 3 4 5 6 7 8 9 10 11 12 13 14  class Solution(object): def inorderTraversal(self, root): \u0026#34;\u0026#34;\u0026#34; :type root: TreeNode :rtype: List[int] \u0026#34;\u0026#34;\u0026#34; self.ans = [] self.dfs(root) return self.ans def dfs(self, node): if not node: return if node.left: self.dfs(node.left) self.ans.append(node.val) if node.right: self.dfs(node.right)   iterative implementation 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  from collections import deque class Solution(object): def inorderTraversal(self, root): \u0026#34;\u0026#34;\u0026#34; :type root: TreeNode :rtype: List[int] \u0026#34;\u0026#34;\u0026#34; if not root: return [] self.stack = deque() ans = [] while root or self.stack: # need add root here while root: self.stack.appendleft(root) root = root.left node = self.stack.popleft() ans.append(node.val) root = node.right return ans   discussion: why the following code is wrong?\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  from collections import deque class Solution(object): def inorderTraversal(self, root): \u0026#34;\u0026#34;\u0026#34; :type root: TreeNode :rtype: List[int] \u0026#34;\u0026#34;\u0026#34; if not root: return [] self.stack = deque() self.stack.appendleft(root) ans = [] while root or self.stack: while root: if root.left: self.stack.appendleft(root.left) root = root.left node = self.stack.popleft() ans.append(node.val) root = node.right return ans   Hint: following edge cases would fail. 1 \\ 2 / 3 When the node 1 is poped out of the stack. The stack is empty. But we still need to explore its right child.\nPost-order The post-order traversal visit nodes in left-right-root order, where the the root appears in the last.\nrecursion implementation 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  class Solution(object): def postorderTraversal(self, root): \u0026#34;\u0026#34;\u0026#34; :type root: TreeNode :rtype: List[int] \u0026#34;\u0026#34;\u0026#34; self.ans = [] self.dfs(root) return self.ans def dfs(self, node): if not node: return [] if node.left: self.dfs(node.left) if node.right: self.dfs(node.right) self.ans.append(node.val)   iterative implementation The idea is to hack the pre-order traversal, which visit nodes in root-left-right order. If we visit in the root-right-left order and reverse it, then we get left-right-root order.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  from collections import deque class Solution(object): def postorderTraversal(self, root): \u0026#34;\u0026#34;\u0026#34; :type root: TreeNode :rtype: List[int] \u0026#34;\u0026#34;\u0026#34; while not root: return [] ans = [] stack = deque() stack.appendleft(root) while stack: # root-right-left order # reverse it; left-right-root node = stack.popleft() ans.append(node.val) if node.left: stack.appendleft(node.left) if node.right: stack.appendleft(node.right) return ans[::-1]   Summary: templates recursion iterative  Reference\nLevel-order From left to right from root to leaf.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  from collections import deque class Solution(object): def levelOrder(self, root): \u0026#34;\u0026#34;\u0026#34; :type root: TreeNode :rtype: List[List[int]] \u0026#34;\u0026#34;\u0026#34; if not root: return [] queue = deque() ans = [] queue.append(root) while queue: node = queue.popleft() ans.append(node.val) if node.left: queue.append(node.left) if node.right: queue.append(node.right) return ans   Variant: level aware method  Leet-code: 102\n给你一个二叉树，请你返回其按 层序遍历 得到的节点值。 （即逐层地，从左到右访问所有节点）。 示例： 二叉树：[3,9,20,null,null,15,7], 3 / \\ 9 20 / \\ 15 7 返回其层次遍历结果： [ [3], [9,20], [15,7] ] 来源：力扣（LeetCode） 链接：https://leetcode-cn.com/problems/binary-tree-level-order-traversal 著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。 The key is to create a variable to record number of nodes in current level.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  from collections import deque class Solution(object): def levelOrder(self, root): \u0026#34;\u0026#34;\u0026#34; :type root: TreeNode :rtype: List[List[int]] \u0026#34;\u0026#34;\u0026#34; if not root: return [] queue = deque() ans = [] queue.append(root) while queue: temp = [] num_nodes_in_current_level = len(queue) while num_nodes_in_current_level \u0026gt; 0: node = queue.popleft() temp.append(node.val) if node.left: queue.append(node.left) if node.right: queue.append(node.right) num_nodes_in_current_level -= 1 # when reach this line, nodes in current level has been all explored ans.append(temp) return ans   ","date":1597536000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597536000,"objectID":"049f471b341c3c175c2c8174320c7554","permalink":"/resources/dsal/tree/tree_basic/","publishdate":"2020-08-16T00:00:00Z","relpermalink":"/resources/dsal/tree/tree_basic/","section":"resources","summary":"Traverse Algorithms Pre-order The pre-order traversal visit nodes in root - left - right order, where the the root appears in the first. recursion implementation 1 2 3 4 5","tags":null,"title":"Basics of tree data structure and algorithms","type":"docs"},{"authors":null,"categories":null,"content":"Smart Pointers - shared pointer   shared pointer  pass shared pointer to function  Pass by Reference and Pass by Pointer   differences between pass-by-reference and pass-by-pointer  ","date":1597104000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598900989,"objectID":"2b661429a92f9ae32adb59dc76e08fa7","permalink":"/resources/cpp_laopt/cpp/pointers/","publishdate":"2020-08-11T00:00:00Z","relpermalink":"/resources/cpp_laopt/cpp/pointers/","section":"resources","summary":"Smart Pointers - shared pointer   shared pointer  pass shared pointer to function  Pass by Reference and Pass by Pointer   differences between pass-by-reference and pass-by-pointer  ","tags":null,"title":"Pointers and Reference","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559407402,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":[],"categories":["shell"],"content":"Find files with specific patterns 1 2 3 4  # This command find files with name has `news20` in the current directory and all of its sub-directories. find . -name \u0026#39;*news20*\u0026#39; # This command find and delete files with name has `news20` in the current directory and all of its sub-directories. find . -name \u0026#39;*news20*\u0026#39; -delete   Show file size 1  ls -l --block-size=M   ","date":1596499200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596741242,"objectID":"f9ca43dbfa4b00c2a6c91ccdbb951369","permalink":"/post/bash-commands-collection/","publishdate":"2020-08-04T00:00:00Z","relpermalink":"/post/bash-commands-collection/","section":"post","summary":"Find files with specific patterns 1 2 3 4  # This command find files with name has `news20` in the current directory and all of its sub-directories. find . -name \u0026#39;*news20*\u0026#39; # This command find and delete files with name has `news20` in the current directory and all of its sub-directories.","tags":[],"title":"bash commands collection","type":"post"},{"authors":["Frank E. Curtis","Yutong Dai","Daniel P. Robinson"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596511776,"objectID":"2a4296a9f86561fc82385c8a924261a1","permalink":"/publication/farsagroup/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/publication/farsagroup/","section":"publication","summary":"We consider the problem of minimizing an objective function that is the sum of a convex function and a group sparsity-inducing regularizer. Problems that integrate such regularizers arise in modern machine learning applications, often for the purpose of obtaining models that are easier to interpret and that have higher predictive accuracy. We present a new method for solving such problems that utilize subspace acceleration, domain decomposition, and support identification. Our analysis shows, under common assumptions, that the iterate sequence generated by our framework is globally convergent, converges to an $\\epsilon$-approximate solution in at most $O(\\epsilon^{-(1+p)})$ (respectively, $O(\\epsilon^{-(2+p)})$) iterations for all $\\epsilon$ bounded above and large enough (respectively, all $\\epsilon$ bounded above) where $p  0$ is an algorithm parameter, and exhibits superlinear local convergence. Preliminary numerical results for the task of binary classification based on regularized logistic regression show that our approach is efficient and robust, with the ability to outperform a state-of-the-art method.","tags":["subspace acceleration","support identification","proximal gradient","second-order method"],"title":"A Subspace Acceleration Method for Minimization Involving a Group Sparsity-Inducing Regularizer","type":"publication"},{"authors":[],"categories":["python"],"content":"Show all submodules I need to import a particular function formulate from a file in the directory \u0026lt;path-to-the-package\u0026gt;/coinor/dippy/examples/milp/milp_func. It\u0026rsquo;s clear that I need to import it from the submodule coinor.dippy. But how to do it exactly? Use following commands, which list all submodules you can import.\n1 2 3 4 5 6 7  import pkgutil import coinor.dippy package=coinor.dippy for importer, modname, ispkg in pkgutil.walk_packages(path=package.__path__, prefix=package.__name__+\u0026#39;.\u0026#39;, onerror=lambda x: None): print(modname)   Relavant outputs are\n1 2 3 4 5  ..... coinor.dippy.examples.milp coinor.dippy.examples.milp.__main__ coinor.dippy.examples.milp.milp_func .....   Then I can simply use\n1  from coinor.dippy.examples.milp.milp_func import formulate   Using the right kernel for Jupyter Notebook 1 2 3 4 5 6 7 8 9 10  # create virtual env with python 3.7.7, whose name is cuppy conda create -n cuppy numpy scipy pandas notebook matplotlib python=3.7.7 # activate cuppy conda activate cuppy # I am using zsh, you may change to bash conda init zsh # activate virtual env cond activate cuppy # point this verison of Python to jupyter ipython kernel install --name \u0026#34;cuppy\u0026#34; --user   Running Jupyter Notebook from the remote server  Reference\n  set up jupyter notebook on login nodes.  set up jupyter notebook on computation nodes   On the server side:\n  Create following two functions in the .bashrc and reload it using source .bashrc\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  function Inode(){ # provide the computation node name; default is polyp2 local nodename=\u0026#34;${1:-polyp2}\u0026#34; echo \u0026#34;starting an interactive section at $nodename\u0026#34; # start an interactive session in the given node qsub -l nodes=$nodename:ppn=4 -l walltime=1:00:00 -l mem=10gb,vmem=10gb -I } function jpt(){ # provide the port; default is 1234 local port=\u0026#34;${1:-1234}\u0026#34; echo \u0026#34;open jupyter notebook at $(hostname):$port\u0026#34; # Fires-up a Jupyter notebook by supplying a specific port and ip jupyter notebook --no-browser --port=$port --ip=$(hostname) }     In the server side\u0026rsquo;s terminal, if\n If you want to start the jupyter notebook in the login node, just call jpt; If you want to start the jupyter notebook in the computation node, call Inode first and then when you are prompted to the computation node, then call jpt. For example, if the comutation node name is polyp3, then call Inode polyp3 and then call jpt 1234.    On the local side:\n  Create following two functions in the .bashrc and reload it using source .bashrc\n1 2 3 4 5 6 7 8 9 10 11 12  function jptt(){ local localport=\u0026#34;${1:-2234}\u0026#34; local servername=\u0026#34;${2:-polyp1}\u0026#34; local serverport=\u0026#34;${3:-1234}\u0026#34; # Forwards port $1 into port $3 and listens to it ssh -N -f -L localhost:$localport:$servername:$serverport yud319@polyps.ie.lehigh.edu } function stopjpt(){ local localport=\u0026#34;${1:-2234}\u0026#34; lsof -i tcp:$localport |awk \u0026#39;NR \u0026gt; 1 {print $2}\u0026#39; | xargs kill -9 echo \u0026#34;Kill port $localport\u0026#34; }     Call jptt on the local terminal, which will listen to the jupyter notebook host on the server.\n  In the browser, if the port on local side is set to 2234, the just type localhost::2234.\n  After finish the job, call stopjpt, which will free the local port.\n  ","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598900989,"objectID":"ee8a896dd688cb4b71cf26f9bb1911ca","permalink":"/post/python-tricks-learned-from-projects/","publishdate":"2020-05-01T00:00:00Z","relpermalink":"/post/python-tricks-learned-from-projects/","section":"post","summary":"Show all submodules I need to import a particular function formulate from a file in the directory \u0026lt;path-to-the-package\u0026gt;/coinor/dippy/examples/milp/milp_func. It\u0026rsquo;s clear that I need to import it from the submodule coinor.dippy. But how to do it exactly?","tags":["python"],"title":"Python Tricks Learned From Projects","type":"post"},{"authors":null,"categories":["Optimization"],"content":" Some useful notes Farkas’s Lemma, Strong Duality and Criss-Cross Algorithm   ","date":1571356800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585430638,"objectID":"46aba06c97713885c14b80b492233241","permalink":"/post/lp/","publishdate":"2019-10-18T00:00:00Z","relpermalink":"/post/lp/","section":"post","summary":" Some useful notes Farkas’s Lemma, Strong Duality and Criss-Cross Algorithm   ","tags":["linear programming"],"title":"Introduction to Mathematical Programming","type":"post"},{"authors":[],"categories":["visualization"],"content":"中国传统计时单位 古时一天分12个时辰，采用地支作为时辰名称，分为\n'子', '丑', '寅', '卯' '辰', '巳', '午', '未' '申', '酉', '戌', '亥' 时辰的起点是午夜，即子初。\n点击下图色块便可查看时间对应。\n 附录\n 参考文献:    国学网  新浪博客 博主知书少年果麦麦  用于绘图的Python代码  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102  # using following code in jupyter notebook import plotly import plotly.graph_objs as go plotly.offline.init_notebook_mode(connected=True) colorPlate1 = [ \u0026#39;#ffb3a7\u0026#39;, \u0026#39;#ffc773\u0026#39;, \u0026#39;#ffa400\u0026#39;, \u0026#39;#c9dd22\u0026#39;, \u0026#39;#afdd22\u0026#39;, \u0026#39;#cca4e3\u0026#39;, \u0026#39;#b0a4e3\u0026#39;, \u0026#39;#ffc64b\u0026#39;, \u0026#39;#ffb61e\u0026#39;, \u0026#39;#758a99\u0026#39;, \u0026#39;#6b6882\u0026#39;, \u0026#39;#ffb3a7\u0026#39;, \u0026#39;#f47983\u0026#39;, \u0026#39;#ffc773\u0026#39;, \u0026#39;#ffa400\u0026#39;, \u0026#39;#c9dd22\u0026#39;, \u0026#39;#afdd22\u0026#39;, \u0026#39;#cca4e3\u0026#39;, \u0026#39;#b0a4e3\u0026#39;, \u0026#39;#ffc64b\u0026#39;, \u0026#39;#ffb61e\u0026#39;, \u0026#39;#758a99\u0026#39;, \u0026#39;#6b6882\u0026#39;, \u0026#39;#f47983\u0026#39; ] colorPlate2 = [\u0026#39;#ff4e20\u0026#39;, \u0026#39;#ff7500\u0026#39;, \u0026#39;#789262\u0026#39;, \u0026#39;#8d4bbb\u0026#39;, \u0026#39;#e9bb1d\u0026#39;,\u0026#39;#50616d\u0026#39;] * 2 theta_marker = [\u0026#34;{}:00\u0026#34;.format(i) for i in range(24)] timeStamp = [ \u0026#39;子正\u0026#39;, \u0026#39;丑初\u0026#39;, \u0026#39;丑正\u0026#39;, \u0026#39;寅初\u0026#39;, \u0026#39;寅正\u0026#39;, \u0026#39;卯初\u0026#39;, \u0026#39;卯正\u0026#39;, \u0026#39;辰初\u0026#39;, \u0026#39;辰正\u0026#39;, \u0026#39;巳初\u0026#39;, \u0026#39;巳正\u0026#39;, \u0026#39;午初\u0026#39;, \u0026#39;午正\u0026#39;, \u0026#39;未初\u0026#39;, \u0026#39;未正\u0026#39;, \u0026#39;申初\u0026#39;, \u0026#39;申正\u0026#39;, \u0026#39;酉初\u0026#39;, \u0026#39;酉正\u0026#39;, \u0026#39;戌初\u0026#39;, \u0026#39;戌正\u0026#39;, \u0026#39;亥初\u0026#39;, \u0026#39;亥正\u0026#39;, \u0026#39;子初\u0026#39; ] timeStampMain = [ \u0026#39;子, 名曰「困敦」\u0026lt;br\u0026gt;混沌万物之初萌，藏黄泉之下。\u0026lt;br\u0026gt; 子是兹的意思，这时候万物刚刚开始滋生和繁殖。\u0026#39;, \u0026#39;丑, 名曰「赤奋若」\u0026lt;br\u0026gt;气运奋迅而起，万物无不若其性。\u0026lt;br\u0026gt;形容万物继续萌发，系于生长。\u0026#39;, \u0026#39;寅, 名曰「摄提格」\u0026lt;br\u0026gt;万物承阳而起。\u0026lt;br\u0026gt;植物芽刚刚吐露，要吸收阳气生长，然后全部露出地面。\u0026#39;, \u0026#39;卯, 名曰「单阏」\u0026lt;br\u0026gt;阳气推万物而起. \u0026lt;br\u0026gt;卯，就是茂，茂盛的样子。这个时候，万物生长滋生繁茂。\u0026#39;, \u0026#39;辰, 名曰「执徐」\u0026lt;br\u0026gt;伏蛰之物，而敷舒出。\u0026lt;br\u0026gt;万物都震动而生长，草木伸舒，萌芽而出。\u0026#39;, \u0026#39;巳, 名曰「大荒落\u0026lt;br\u0026gt;万物炽盛而出，霍然落之。\u0026lt;br\u0026gt;万物到了这个时候，都全部长起来了，聚集在一起。炽盛而有光泽的样子。\u0026#39;, \u0026#39;午, 名曰「敦牂」\u0026lt;br\u0026gt;万物壮盛也。\u0026lt;br\u0026gt;万物都达到盛大壮茂，枝柯密布的状态。\u0026#39;, \u0026#39;未, 名曰「协洽」\u0026lt;br\u0026gt;阴阳和合，万物化生。\u0026lt;br\u0026gt;未，就是味的意思。当事物成熟的时候，都会发出气味。这时候阴气开始升起，万物稍微衰败。\u0026#39; , \u0026#39;申, 名曰「涒滩」\u0026lt;br\u0026gt;万物吐秀，倾垂也。\u0026lt;br\u0026gt;万物的身体都已成就，倾吐了最后的繁盛，引向衰败。\u0026#39;, \u0026#39;酉, 名曰「作噩」\u0026lt;br\u0026gt;万物皆芒枝起。 \u0026lt;br\u0026gt;万物衰老到极至而成熟。\u0026#39;, \u0026#39;戌, 名曰「阉茂」\u0026lt;br\u0026gt;万物皆蔽冒也。\u0026lt;br\u0026gt;戌，灭，杀的意思。意思是到了这时候，万物都已经衰灭了。\u0026#39;, \u0026#39;亥, 名曰「大渊献」\u0026lt;br\u0026gt;万物于天，深盖藏也。\u0026lt;br\u0026gt;亥，核的意思。万物都进入核阂里，意味着阴气劾杀了万物，等待下一个初萌。\u0026#39; ] def sectorChild(name, location, radius, fillcolor=\u0026#39;#ff4e20\u0026#39;): r = [0] * 24 r[location % 24] = radius r[(location + 1) % 24] = radius obj = go.Scatterpolar( name = name, r = r, theta = theta_marker, fill = \u0026#34;toself\u0026#34;, fillcolor = fillcolor, line = {\u0026#34;color\u0026#34;:\u0026#39;black\u0026#39;} ) return obj def sectorParent(name, location, radius, fillcolor=\u0026#39;black\u0026#39;): r = [0] * 24 if location == 0: r[0] = r[1] = r[23] = radius else: r[location % 24] = radius r[(location + 1) % 24] = radius r[(location + 2) % 24] = radius obj = go.Scatterpolar( name = name, r = r, theta = theta_marker, fill = \u0026#34;toself\u0026#34;, fillcolor = fillcolor, line = {\u0026#34;color\u0026#34;:\u0026#39;black\u0026#39;}, hoverinfo = \u0026#39;text\u0026#39;, hoverlabel = {\u0026#39;align\u0026#39;:\u0026#39;left\u0026#39;} ) return obj trace = [sectorChild(i, j, 5, k) for (i,j,k) in zip(timeStamp, range(24), colorPlate1)] trace += [sectorParent(timeStampMain[0], 0, 3, colorPlate2[0])] trace += [sectorParent(i, j, 3, k) for (i,j, k) in zip(timeStampMain[1:], range(1, 23, 2), colorPlate2[1:]) ] layout = go.Layout( polar = dict( radialaxis = dict( visible = False ), angularaxis = dict( direction = \u0026#34;clockwise\u0026#34;, visible = True, linewidth = 3 ) ), showlegend = False ) plotly.offline.iplot({ \u0026#34;data\u0026#34;: trace, \u0026#34;layout\u0026#34;: layout })   ","date":1562284800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1566272759,"objectID":"a78143e43dd46b5640ab9af8fd0b0dd5","permalink":"/post/chinese-time/","publishdate":"2019-07-05T00:00:00Z","relpermalink":"/post/chinese-time/","section":"post","summary":"中国传统计时单位 古时一天分12个时辰，采用地支作为时辰名称，","tags":["visualization"],"title":"中国传统计时","type":"post"},{"authors":[],"categories":["optimization"],"content":"Problem Formulation There are commonly two ways of formulating the logistic regression problem, depending on the way we label the response variable $y$. Here we focus on the first formulation and defer the second formulation on the appendix.\nFirst Formulation:\nConsider restrict $y$ to {${-1,1}$}. Then we have $$ \\begin{aligned} \u0026amp;\\mathbb{P}(y=1|z)=\\sigma(z)=\\frac{1}{1 + e^{-z}}\\\n\u0026amp;\\mathbb{P}(y=-1|z)=\\sigma(z)=\\frac{1}{1 + e^z}, \\end{aligned} $$ which can be compactly written as $$ \\mathbb{P}(y|z)=\\sigma(zy). $$ If we consider the data $({x_i,y_i})_{i=1}^N$ and we want to use the Likelihood Principle to fit the Logistic Regression, then we would like to maximize the following loss function, $$ \\begin{aligned} \u0026amp; L(\\beta_0,\\beta) = \\prod_{i=1}^N \\mathbb{P}(y_i|z_i)\\\n\u0026amp; z_i =\\beta_0+\\beta^Tx_i. \\end{aligned} $$ If we use the first formulation, then it is equivalent to minimize the log-negative of $L(\\beta_0,\\beta)$, $$ \\begin{aligned} \\min_{\\beta_0,\\beta}l(\\beta_0,\\beta)=\\frac{1}{N}\\sum_{i=1}^N\\log(1+e^{-y_iz_i}). \\end{aligned} $$ From now on, for the sake of simplicity, we drop the intercept term $\\beta_0$.\nMotivating Example Consider two simulated datasets:\nDataset 1:\n   $x_1$ $x_2$ $y$     0.3 0.9 1   0.5 1.5 -1    Dataset 2:\n   $x_1$ $x_2$ $y$     2 1 1   -1 -1 -1    Some Analysis:\n The objective function $l(\\beta)$ is strictly convex by looking at its Hessian, which is positive defined. However, it is not strongly convex. For given Data set, the Hessian is upper bounded by $(\\sum_{i=1}^N|x_i|^2)I$ (see Appendix). The stepsize can be chosen as $\\alpha = \\frac{1}{\\sum_{i=1}^N|x_i|^2}$.  Applying the gradient descent with constant stepsize $\\frac{1}{L}$ on each dataset for 1000 steps, then we obtain the estimations as follows.\n   Dataset $\\beta_1$ $\\beta_2$     1 -0.12058225 -0.36174676   2 3.59370507 3.04825501    Also we plot out following figures to check the convergence. The top two figures describe the algorithm\u0026rsquo;s performance on the dataset 1 while the bottom two is for the dataset 2.\n   Fig1: Apply GD with the constant stepsize on two different datasets. The blue curves depicts how the norm of gradient at iterates change while the red curves show the change of the function value in each iteration.\n Analysis: why this happens? First, if we want to minimize $f(\\beta)=\\log(1 + \\exp(-\\beta))$ using gradient descent with constant stepsize $\\frac{1}{L}$, then we will facing following issues. Here we assume $\\beta \\in \\mathbb{R}$.\n The global minimal is not attainable, i.e., $+\\infty$, though we can have $\\nabla f(\\beta^k)\\rightarrow 0$, which means $\\beta \\rightarrow +\\infty$, hence the iterates diverge. Indeed, the ${f(\\beta^k)}$ converges to $f^*=0$ by as it monotonously decreasing and lower bounded by $0$. The worst-case iteration complexity is $\\mathcal{O}(\\frac{1}{k})$, indicating a sublinear convergence rate.  Now, let\u0026rsquo;s back to the example. The figure 2 shows that the first dataset and second dataset, which correspond to the non-separable and separable case respectively.\n   Fig2: (Left) First dataset. (Right) Second dataset. The fitted separating line is derived by $y=-\\frac{\\beta_1}{\\beta_2}x$.\n We also plot out the norm of iterates at each iteration in figure 3.\n   Fig3: The top figure shows the norm of iterates for the first dataset while the bottom one shows case for the second dataset.\n We can see that for non-separable case, the norm of iterates are bounded while the latter goes to infinity (if we increase the number of iterations).\nIn non-separable case, ${\\beta^k}$ seems to stay in \u0026ldquo;strongly convex\u0026rdquo; region while in separable case, ${\\beta^k}$ keeps approaching the flatten region, so you can easily say a sharp decreasing in convergence speed. The following observations can be verified by figure 4 (1-dimensional case) and figure 5 (2-dimensional case).\n   Fig4: (Left) Non-separable dataset ${(x_1=1, y_1=1), (x_2=2, y_2=-1)}$. The green dot line is $y=x^2$. The objective function (blue line) preserves the strong convexity in a certain range and the minimal stays in this range. The red point is the start point $x_0$. (Right). Separable dataset ${(x_1=1, y_1=1), (x_2=-1, y_2=-1)}$. Although the objective function is endowed with the strong convexity property in a certain range, however the global minimal is outside of this range.\n    Fig5: Dataset 1 is shown in the top 2 pictures with the right one zooming into a particular range. Dataset 2 is shown in the bottom pictures. The blue dots trace the progression of iterates.\n Questions  Why the separability would cause such a difference? From the Fig4 and Fig5, we know data as parameters can influence the shape of the objective function a lot. Given the data set, can we predict the behavior of the performance of gradient descent with constant stepsize, i.e., linear convergence rate or sublinear convergence rate? Can we extend our conclusion to higher dimension? In real world application, it\u0026rsquo;s likely that the data is semi-separable, i.e., most data points can be split into two groups with a few exceptions. How\u0026rsquo;s that influence the performance of the algorithm? Will second formulation (see below) also encounter the similar issue? My guess is yes.  Appendix Second Formulation of Logistic Regression\nConsider restrict $y$ to ${0,1}$. Then we have $$ \\begin{aligned} \u0026amp;\\mathbb{P}(y=1|z)=\\sigma(z)=\\frac{1}{1 + e^{-z}}\\\n\u0026amp;\\mathbb{P}(y=0|z)=\\sigma(z)=\\frac{1}{1 + e^z}, \\end{aligned} $$ which can be compactly written as $$ \\mathbb{P}(y|z)=\\sigma(z)^y(1-\\sigma(z))^{1-y}. $$\nIf we use the second formulation, then maximizing the likelihood is equivalent to $$ \\begin{aligned} \\min_{\\beta_0,\\beta}l(\\beta_0,\\beta)=\\frac{1}{N}\\sum_{i=1}^N[-y_iz_i+\\log(1+e^{z_i})]. \\end{aligned} $$\nDerivation of the gradient and Hessian of the loss function (first formualtion)\nConsider $f(\\beta)=\\log (1 + \\exp(-y\\beta^Tx)$, then we have\n$$ \\begin{aligned} \u0026amp;\\nabla f(\\beta) = \\frac{1}{1 + \\exp(y\\beta^Tx)}(-yx)\\\n\u0026amp;\\nabla^2 f(\\beta) = (yx)\\frac{\\exp(y\\beta^Tx)}{1 + \\exp(y\\beta^Tx)}(yx^T), \\end{aligned} $$ which implies $$ \\begin{aligned} \u0026amp; \\nabla l(\\beta)=\\frac{1}{N}\\sum_{i=1}^N \\frac{1}{1 + \\exp(y_i\\beta^Tx_i)}(-y_ix_i)\\\n\u0026amp; \\nabla^2 l(\\beta)=\\frac{1}{N}\\sum_{i=1}^N(y_ix_i)\\frac{\\exp(y\\beta^Tx_i)}{(1 + \\exp(y\\beta^Tx_i))^2}(y_ix_i^T)=\\frac{1}{N}XDX^T, \\end{aligned} $$ where $X=[x_1,\\cdots,x_n]$ , $D=\\text{diag}({y_1^2\\sigma_1(1-\\sigma_1),\\cdots,y_n^2\\sigma_n(1-\\sigma_n)})$ , and $\\sigma_i =\\frac{\\exp(y\\beta^Tx_i)}{1 + \\exp(y\\beta^Tx_i)} $.\nReference:\n  Lecture notes 9 and 10 presented on this course website.\n  The code for generating graphs can be found in my git repo.\n  ","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596511776,"objectID":"473ae1c2728e1a3326fc148defe204ad","permalink":"/post/gradient-descent-in-logistic-regression/","publishdate":"2019-07-01T00:00:00Z","relpermalink":"/post/gradient-descent-in-logistic-regression/","section":"post","summary":"Problem Formulation There are commonly two ways of formulating the logistic regression problem, depending on the way we label the response variable $y$. Here we focus on the first formulation and defer the second formulation on the appendix.","tags":[],"title":"Gradient Descent in Logistic Regression","type":"post"},{"authors":["Yutong Dai","Yang Weng"],"categories":null,"content":"","date":1554076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596505925,"objectID":"f934a9b2b0fdb05ac23f1c7f653ed912","permalink":"/publication/psum/","publishdate":"2019-04-01T00:00:00Z","relpermalink":"/publication/psum/","section":"publication","summary":"In this paper, we propose a synchronous parallel block coordinate descent algorithm(PSUM) for minimizing a composite function, which consists of a smooth convex function plus a non-smooth but separable convex function. Due to the generalization of our method, some existing synchronous parallel algorithms can be considered as special cases. To tackle high dimensional problems, we further develop a randomized variant, which randomly update some blocks of coordinates at each round of computation. Both proposed parallel algorithms are proven to have sub-linear convergence rate under rather mild assumptions. The numerical experiments on solving the large scale regularized logistic regression with $l_1$ norm penalty show that the implementation is quite efficient. We conclude with explanation on the observed experimental results and discussion on the potential improvements.","tags":["Convex Optimization","Block Cooridinate Descent"],"title":"Synchronous Parallel Block Coordinate Descent Method for Nonsmooth Convex Function Minimization","type":"publication"},{"authors":[],"categories":[],"content":"Welcome to Slides  Academic\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export: E   Code Highlighting Inline code: variable\nCode block:\n1 2 3  porridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;)    Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n1 2 3 4  {{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}   Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n1 2 3  {{\u0026lt; slide background-image=\u0026#34;/img/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}}    Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n1 2 3 4 5  .reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }    Questions?  Ask\n Documentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559407402,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":["Statistics"],"content":"  1 Point Anomaly Detection - Grubbs' test 2 Collective Anomaly Detection 2.1 Anomaly in timeseries - Seasonal Hybrid ESD algorithm 2.2 Distance-based Anomaly Detection 2.2.1 Global Anomaly - Largest Distance 2.2.2 Local Anomaly - LOF   3 Isolation Forest 3.1 Isolation Score    1 Point Anomaly Detection - Grubbs' test Grubbs' test1 is commonly used technique to detect an outlier in univariate problem, where normality assumption is required. It can be formualted as either one-side testing problem or two-sided testing problem.\nThe hypothesis test is defined as\n\\[H_0: \\text{There are no outlier in the data set} \\quad H_1: \\text{There is exactly one outlier in the data set}.\\] For two-sided testing, it tries to determine whether the observation with the largest absolute deviation is an outlier, where the test statistic is defined as\n\\[ G = \\frac{\\max_i |X_i - \\bar X|}{s}, \\] where the \\(\\bar X\\) is the sample mean and \\(s\\) is the sample deviation.\nLet's look at one simulated example.\nset.seed(123) simulated_data \u0026lt;- rnorm(100, 0, 1) simulated_data_with_outliers \u0026lt;- c(simulated_data, c(3.5, -3.7)) # normality check shapiro.test(simulated_data_with_outliers) ## ## Shapiro-Wilk normality test ## ## data: simulated_data_with_outliers ## W = 0.9804, p-value = 0.1344 Shapiro-Wilk normality test impiles the data is normally distributed. Now, let's performe the grubbs' test.\nlibrary(outliers) grubbs.test(simulated_data_with_outliers, two.sided = TRUE) ## ## Grubbs test for one outlier ## ## data: simulated_data_with_outliers ## G = 3.65380, U = 0.86651, p-value = 0.01626 ## alternative hypothesis: lowest value -3.7 is an outlier The test result detecs the lowest value as an outlier.\nLet's remove the -3.7 and performe the test again.\ngrubbs.test(head(simulated_data_with_outliers,101), two.sided = TRUE) ## ## Grubbs test for one outlier ## ## data: head(simulated_data_with_outliers, 101) ## G = 3.48190, U = 0.87755, p-value = 0.03378 ## alternative hypothesis: highest value 3.5 is an outlier Finally, let's remove two outliers altogether.\ngrubbs.test(head(simulated_data_with_outliers,100), two.sided = TRUE) ## ## Grubbs test for one outlier ## ## data: head(simulated_data_with_outliers, 100) ## G = 2.62880, U = 0.92949, p-value = 0.7584 ## alternative hypothesis: lowest value -2.30916887564081 is an outlier This impiles there are no outliers.\nGrubbs' test is useful for identify the outliers of a small amount one at a time, but not suitable to detect a group of outliers.\n 2 Collective Anomaly Detection 2.1 Anomaly in timeseries - Seasonal Hybrid ESD algorithm # devtools::install_github(\u0026quot;twitter/AnomalyDetection\u0026quot;) library(AnomalyDetection) river \u0026lt;- read.csv(\u0026quot;https://raw.githubusercontent.com/Rothdyt/personal-blog/master/static/post/dataset/river.csv\u0026quot;) results \u0026lt;- AnomalyDetectionVec(river$nitrate, period=12, direction = \u0026#39;both\u0026#39;, plot = T) results$plot  2.2 Distance-based Anomaly Detection 2.2.1 Global Anomaly - Largest Distance Intuitively, the larger distance the more likely the point would be an outlier.\nlibrary(FNN) ## Warning: package \u0026#39;FNN\u0026#39; was built under R version 3.4.4 furniture \u0026lt;- read.csv(\u0026quot;https://raw.githubusercontent.com/Rothdyt/personal-blog/master/static/post/dataset/furniture.csv\u0026quot;) furniture_scaled \u0026lt;- data.frame(Height = scale(furniture$Height), Width = scale(furniture$Width)) furniture_knn \u0026lt;- get.knn(furniture_scaled, k = 5) furniture_scaled$score_knn \u0026lt;- rowMeans(furniture_knn$nn.dist) largest_idx \u0026lt;- which.max(furniture_scaled$score_knn) plot(furniture_scaled$Height, furniture_scaled$Width, cex=sqrt(furniture_scaled$score_knn), pch=20) points(furniture_scaled$Height[largest_idx], furniture_scaled$Width[largest_idx], col=\u0026quot;red\u0026quot;, pch=20)  2.2.2 Local Anomaly - LOF kNN is useful for finding global anomalies, but is less able to surface local outliers.\nLOF is a ratio of densities:\n LOF \u0026gt; 1 more likely to be anomalous LOF ≤ 1 less likely to be anomalous Large LOF values indicate more isolated points  library(dbscan) furniture_lof \u0026lt;- furniture[,2:3] furniture_lof$score_lof \u0026lt;- lof(scale(furniture_lof), k=5) largest_idx \u0026lt;- which.max(furniture_lof$score_lof) plot(furniture_lof$Height, furniture_lof$Width, cex=sqrt(furniture_lof$score_lof), pch=20) points(furniture_lof$Height[largest_idx], furniture_lof$Width[largest_idx], col=\u0026quot;red\u0026quot;, pch=20) It's clear that, lof successfuly detects the local outlier.\n   3 Isolation Forest  Isolation Forest is built on the basis of decision trees; To grow a decision tree, at each node, a feature and a corresponding cutoff value are randomly selected; Intuitively, outliers are less frequent than regular observations and are different from them in terms of values, so outliers should be identified closer to the root of the tree with fewer splits. We use isolation score to characterize this.  3.1 Isolation Score We need some quatity to define the isolation score2\n Path Length: \\(h(x)\\) of a point \\(x\\) is measured by the number of edges \\(x\\) traverses an iTree from the root node until the traversal is terminated at an external node. Normalizing constant \\[c(n) = 2H(n − 1) − (2(n − 1)/n)\\], where \\(n\\) is the number of samples to grow a tree and \\(H(i)\\) is the harmonic number and it can be estimated by \\(ln(i) + 0.5772156649\\) (Euler’s constant).  The isolation score \\(s\\) of an sample \\(x\\) is defined as \\[s(x,n)= 2^{-\\frac{E(h(x))}{c(n)}},\\] where the \\(E()\\) is the expectation of \\(h(x)\\).\n Interpreting the isolation score:\n Scores between 0 and 1 Scores near 1 indicate anomalies (small path length)\n  # devtools::install_github(\u0026quot;Zelazny7/isofor\u0026quot;) library(isofor) furniture \u0026lt;- read.csv(\u0026quot;https://raw.githubusercontent.com/Rothdyt/personal-blog/master/static/post/dataset/furniture.csv\u0026quot;) furniture \u0026lt;- data.frame(Height = furniture$Height, Width = furniture$Width) scores \u0026lt;- matrix(nrow=dim(furniture)[1]) for (ntree in c(100, 200, 500)){ furniture_tree \u0026lt;- iForest(furniture, nt = ntree, phi=50) scores \u0026lt;- cbind(scores, predict(furniture_tree, furniture)) } plot(scores[,3], scores[,4], xlab = \u0026quot;200 tress\u0026quot;, ylab=\u0026quot;500 tress\u0026quot;) abline(a=0,b=1) This graph is used to assess wheter the number of trees is enough for the isolation score to converge. From the graph above, we know that 200 tress are enough for us to identify the anomalies.\nlibrary(lattice) furniture_forest \u0026lt;- iForest(furniture, nt = 200, phi=50) h_seq \u0026lt;- seq(min(furniture$Height), max(furniture$Height), length.out = 20) w_seq \u0026lt;- seq(min(furniture$Width), max(furniture$Width), length.out = 20) furniture_grid \u0026lt;- expand.grid(Width = w_seq, Height = h_seq) furniture_grid$score \u0026lt;- predict(furniture_forest, furniture_grid) contourplot(score ~ Height + Width, data = furniture_grid,region = TRUE) This contour graph used to identify the anomaly regions.\n   https://en.wikipedia.org/wiki/Grubbs%27_test_for_outliers↩\n Zhihua Zhou et al. https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf↩\n   ","date":1546560000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596511776,"objectID":"0f9c92b28639dcbc1da16b2bc8ac9897","permalink":"/post/anomaly-detection/","publishdate":"2019-01-04T00:00:00Z","relpermalink":"/post/anomaly-detection/","section":"post","summary":"1 Point Anomaly Detection - Grubbs' test 2 Collective Anomaly Detection 2.1 Anomaly in timeseries - Seasonal Hybrid ESD algorithm 2.2 Distance-based Anomaly Detection 2.2.1 Global Anomaly - Largest Distance 2.","tags":["data-analysis"],"title":"Anomaly Detection","type":"post"},{"authors":null,"categories":["Python-Programming"],"content":"  Prepare a fitted random forest Find the path to desired terminal node Collect Paths in the random forest Summarize the decison region   Prepare a fitted random forest import random import pandas as pd from sklearn.ensemble.forest import RandomForestRegressor from sklearn import tree data = pd.DataFrame({\u0026quot;Y\u0026quot;:[1,5,3,4,3,4,2], \u0026quot;X_1\u0026quot;:[\u0026quot;red\u0026quot;, \u0026quot;blue\u0026quot;, \u0026quot;blue\u0026quot;, \u0026quot;red\u0026quot;,\u0026quot;red\u0026quot;,\u0026quot;blue\u0026quot;, \u0026quot;red\u0026quot;], \u0026quot;X_2\u0026quot;:[18.4, 7.5, 9.3, 3.7, 5.2, 3.2, 5.2]}) data = pd.get_dummies(data) X = data.drop([\u0026quot;Y\u0026quot;], axis=1) y = data[\u0026quot;Y\u0026quot;] rf = RandomForestRegressor(n_estimators = 10, random_state = 1234) rf.fit(X, y) output:\nRandomForestRegressor(bootstrap=True, criterion=\u0026#39;mse\u0026#39;, max_depth=None, max_features=\u0026#39;auto\u0026#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1, oob_score=False, random_state=1234, verbose=0, warm_start=False)  Find the path to desired terminal node import pydotplus import re def return_node_path_to_max_prediction(onetree, verbose=True): \u0026quot;\u0026quot;\u0026quot; @input: a tree from the sklearn randomforest @output: the node path to maxmium terminal node [[split_node_1], [split_node_2], ...] [splite_node_1] = [var_index, cutoff, direction] \u0026quot;\u0026quot;\u0026quot; if verbose: print(\u0026quot;Generating Tree Graph, it may take a while...\u0026quot;) dot_data = tree.export_graphviz(onetree, out_file = None, filled = True, rounded = True, special_characters = True) graph = pydotplus.graph_from_dot_data(dot_data) graph_ = {} for edge in graph.get_edge_list(): graph_[edge.get_source()] = edge.get_destination() # find all terminal node terminal_node = {} non_decimal = re.compile(r\u0026#39;[^\\d.]+\u0026#39;) for node in graph.get_node_list(): if node.get_name() not in graph_: if node.get_name() not in [\u0026quot;node\u0026quot;, \u0026quot;edge\u0026quot;]: value = node.get_label() value = re.sub(r\u0026#39;.*v\u0026#39;, \u0026#39;v\u0026#39;, value) terminal_node[node.get_name()] = float(non_decimal.sub(\u0026#39;\u0026#39;, value)) # find the path down to the terminal with maximum predition value flag = True destination = max(terminal_node, key=terminal_node.get) edge_list = graph.get_edge_list() node_list = graph.get_node_list() split_node = [] while flag: myedge = [edge for edge in edge_list if edge.get_destination() == destination][0] if int(myedge.get_destination()) - int(myedge.get_source()) \u0026gt; 1: direction = \u0026quot;Right\u0026quot; else: direction = \u0026quot;Left\u0026quot; mynode = [node for node in node_list if node.get_name() == myedge.get_source()][0] var_val = re.findall(r\u0026quot;[-+]?\\d*\\.\\d+|\\d+\u0026quot;, mynode.get_label())[:2] # record the growing path: # var_val[0]: Index of variable participating in splitting # var_val[1]: cutoff point of the splitting # direction: If Right, means greater than var_val[1]; # If Left, means no greater than var_val[1] split_node.append([int(var_val[0]),float(var_val[1]),direction]) if verbose: print(myedge.get_destination() + \u0026quot;\u0026lt;-\u0026quot; + myedge.get_source() + \u0026quot;: Split at Variable X\u0026quot; + var_val[0] + \u0026quot;; The cutoff is \u0026quot; + var_val[1] + \u0026quot;; Turn \u0026quot; + direction) destination = myedge.get_source() if destination == \u0026quot;0\u0026quot;: flag = False return [*reversed(split_node)] Test:\nreturn_node_path_to_max_prediction(rf[1], verbose=True) Outputs:\nGenerating Tree Graph, it may take a while... 3\u0026lt;-1: Split at Variable X0; The cutoff is 5.6; Turn Right 1\u0026lt;-0: Split at Variable X0; The cutoff is 12.95; Turn Left From the output above, we know the path from the root to the desired terminal node is :\nRoot[X0(\u0026lt;= 12.95)] -\u0026gt; X0 (\u0026gt;=5.6) -\u0026gt; Terminal Node\n Collect Paths in the random forest def collect_path(rf, verbose=True): n_tree = len(rf) result = [] for i in range(n_tree): if verbose: print(\u0026quot;Construct the %s tree graph out of %s trees\u0026quot; %(i+1, n_tree)) result.append(return_node_path_to_max_prediction(rf.estimators_[i], verbose=False)) return result Test:\nresult = collect_path(rf) print(result) Outputs:\nConstruct the 1 tree graph out of 10 trees Construct the 2 tree graph out of 10 trees Construct the 3 tree graph out of 10 trees Construct the 4 tree graph out of 10 trees Construct the 5 tree graph out of 10 trees Construct the 6 tree graph out of 10 trees Construct the 7 tree graph out of 10 trees Construct the 8 tree graph out of 10 trees Construct the 9 tree graph out of 10 trees Construct the 10 tree graph out of 10 trees [[[0, 4.2, \u0026#39;Left\u0026#39;]], [[0, 12.95, \u0026#39;Left\u0026#39;], [0, 5.6, \u0026#39;Right\u0026#39;]], [[1, 0.5, \u0026#39;Right\u0026#39;], [0, 8.4, \u0026#39;Left\u0026#39;]], [[0, 13.85, \u0026#39;Left\u0026#39;], [0, 8.4, \u0026#39;Left\u0026#39;], [1, 0.5, \u0026#39;Right\u0026#39;]], [[0, 8.4, \u0026#39;Left\u0026#39;], [0, 6.35, \u0026#39;Right\u0026#39;]], [[0, 12.95, \u0026#39;Left\u0026#39;], [0, 5.6, \u0026#39;Right\u0026#39;]], [[2, 0.5, \u0026#39;Left\u0026#39;], [0, 5.35, \u0026#39;Right\u0026#39;]], [[1, 0.5, \u0026#39;Right\u0026#39;], [0, 5.35, \u0026#39;Right\u0026#39;]], [[0, 13.85, \u0026#39;Left\u0026#39;], [1, 0.5, \u0026#39;Right\u0026#39;], [0, 8.4, \u0026#39;Left\u0026#39;], [0, 5.35, \u0026#39;Right\u0026#39;]], [[0, 13.85, \u0026#39;Left\u0026#39;], [0, 6.35, \u0026#39;Right\u0026#39;], [0, 8.4, \u0026#39;Left\u0026#39;]]]  Summarize the decison region def summarize_region(result, features): decision_region = {k: [[] for _ in range(2)] for k in features} for i in range(len(result)): for j in range(len(result[i])): if result[i][j][2] == \u0026quot;Left\u0026quot;: decision_region[features[result[i][j][0]]][0].append(result[i][j][1]) else: decision_region[features[result[i][j][0]]][1].append(result[i][j][1]) decision_region_ = {} for k in features: try: upper_bound = min(decision_region[k][0]) except ValueError: upper_bound = \u0026quot;Unknown\u0026quot; try: lower_bound = max(decision_region[k][1]) except ValueError: lower_bound = \u0026quot;Unknown\u0026quot; decision_region_[k] = [lower_bound, upper_bound] value_to_remove = [\u0026#39;Unknown\u0026#39;, \u0026#39;Unknown\u0026#39;] decision_region_ = {key: value for key, value in decision_region_.items() if value != value_to_remove} value_to_remove = [0.5, 0.5] decision_region_ = {key: value for key, value in decision_region_.items() if value != value_to_remove} return (decision_region_) Test:\nfeatures = X.columns summarize_region(result, features) Outputs:\n{\u0026#39;X_1_blue\u0026#39;: [0.5, \u0026#39;Unknown\u0026#39;], \u0026#39;X_1_red\u0026#39;: [\u0026#39;Unknown\u0026#39;, 0.5], \u0026#39;X_2\u0026#39;: [6.35, 4.2]} From the output above, we know that the decision region:\n{blue} * [6.35, 4.2] But it seems that the region [6.35, 4.2] is not reasonable due to the poorly generated data. But it may happens in some situations, which may require us to come up with new ways to ensemble these terminal nodes.\n ","date":1530921600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559407402,"objectID":"0093d29b100375be265b5486a11eaca1","permalink":"/post/decision-tree-how-to-find-the-path-from-the-root-to-the-desired-terminal-node/","publishdate":"2018-07-07T00:00:00Z","relpermalink":"/post/decision-tree-how-to-find-the-path-from-the-root-to-the-desired-terminal-node/","section":"post","summary":"Find terminal nodes in each tree of the built random forest that give the largest prediction. Then find paths from root to these selected terminal nodes and ensemble them to derive a decision region.","tags":["RandomForest"],"title":"Decision Tree: How to find the path from the root to the desired terminal node","type":"post"},{"authors":null,"categories":null,"content":"In this project, we\n Develope data products to help Airbnb hosts to determine listing prices using Sparse Regression and RandomForest Researched how amenities and geolocation in uence listing prices Designed a User Interface for customers to gain insight into Airbnb rental markets in Boston  The Video Presentation and Rshiny App Demo are also provided.\n","date":1512882000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546015024,"objectID":"690845e8dc7a3b2b3563f360194fbc75","permalink":"/project/boston-housing/","publishdate":"2017-12-10T00:00:00-05:00","relpermalink":"/project/boston-housing/","section":"project","summary":"Develope data products to help Airbnb hosts to determine listing prices.","tags":["Data Analysis"],"title":"Real Estate Market Data Analysis","type":"project"},{"authors":null,"categories":null,"content":"This is the deep learning course final project trying to reporduce the results reported in the paper, Show and Tell: A Neural Image Caption Generator.\nThe model is trained on the MS coco2014 dataset. Our final result looks like\nFor code, UI, and report please click here.\n","date":1512882000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559407402,"objectID":"217bfb70762bb4746b8027e79602a247","permalink":"/project/show-and-tell/","publishdate":"2017-12-10T00:00:00-05:00","relpermalink":"/project/show-and-tell/","section":"project","summary":"Feed in an image, AI will generate the caption for you!","tags":["Deep Learning","CNN","LSTM"],"title":"Show and Tell: A Neural Image Caption Generator","type":"project"},{"authors":null,"categories":null,"content":"This is the statistical computing course final project, trying to understand, reporduce and extend some results reported in the paper, Variational Inference: A Review for Statisticians.\nThree datasets are used here, simulated data, old faithful and imageCLEF.\nOur final result looks like\n Simulated data    old faithful   imageclef\n  For code and report please click here.\n","date":1512882000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559407402,"objectID":"ef4fca63f9151ea900f34782ac9bfb47","permalink":"/project/variational-inference/","publishdate":"2017-12-10T00:00:00-05:00","relpermalink":"/project/variational-inference/","section":"project","summary":"Clustering under the variation inference framework.","tags":["Data Analysis"],"title":"Variational Gaussian Mixtures","type":"project"},{"authors":null,"categories":["optimization"],"content":"  Problem description Notations Assumption Algorithm  Convergence Analysis Powell's example R codes for numerical experiments   We mainly focus on the convergence of Block coordinate decent with exact minimization, whose block update strategy employs Gauss-Seidel manner. And then use Powell's example to see what will happen if some conditions are not met.\n Reference: 1. Dimitri .P Bertsekas, Nonlinear Programming 2ed 2. Powell ,1973, ON SEARCH DIRECTIONS FOR MINIMIZATION ALGORITHMS\n Problem description Notations We want to solve the problem:\n\\[\\mathop{min}_{x\\in X}\\quad f(x)\\]\nwhere X is a Cartesian product of closed convex sets $X_1,...,X_m:X=_{i=1}^n X_i $\nWe assume that \\(X_i\\) is a closed convex subset of \\(R^{n_i}\\) and \\(n=\\sum_{i=1}^m n_i\\). The vector is partitioned into \\(m\\) block(s) such that \\(x_i \\in X^{n_i}\\).\nWe denote \\(\\nabla_i f\\) as the gradient of \\(f\\) with respect to component \\(x_i\\).\n Assumption We shall assume that for every \\(x\\in X\\) and \\(i=1,2,...m\\) the optimization problem\n\\[\\mathop{min}_{\\xi\\in X_i}\\quad f(x_1,...,x_{i-1},\\xi,x_{i+1,....,x_m})\\]\nhas at least one solution.\n Algorithm The Gauss-Seidel method, generates the next iterate \\(x^{k+1}=(x^{k+1}_1,...,x^{k+1}_m)\\), given the current the iterate \\(x^{k}=(x^{k}_1,...,x^{k}_m)\\), according to the iteration\n\\[x^{k+1}_i=\\mathop{argmin}_{\\xi\\in X_i}\\quad f(x_1^{k+1},...,x^{k+1}_{i-1},\\xi,x^k_{i+1},...,x_m^k)\\]\n  Convergence Analysis Theorem Suppose that \\(f\\) is continuously differentiable over the set \\(X\\) defined as above. Furthermore, suppose that for each \\(i\\) and \\(x\\in X\\),\n\\[f(x_1,...,x_{i-1},\\xi,x_{i+1,....,x_m})\\]\nviewed as a function of \\(\\xi\\), attains a unique minimum \\(\\bar x_i\\) over \\(X_i\\) and is monotonically non-increasing in the interval from \\(x_i\\) to \\(\\bar \\xi\\). Let \\(\\{x_k\\}\\) be the sequence generated by the block coordinate method with Gauss-Seidel manner. Then, every limit point of \\(\\{x_k\\}\\) is a stationary point.\nPROOF\nLet\n\\[z_i^k=(x_1^{k+1},...,x_i^{k+1},x_{i+1}^k,...,x_m^k)\\]\nBy the nature of this algorithm, for all \\(k\\geq 0\\), we have following inequality\n\\[f(x^k)\\geq f(z_1^k)\\geq f(z_2^k)\\geq ...\\geq f(z_{m-1}^k)\\geq f(x^{k+1}) \\quad (*)\\]\nSince \\(\\{x_k\\}in X\\), we can assume \\(\\{x^{k_j}\\}\\) is the subsequence that converges to \\(\\bar x=(\\bar x_1,..,\\bar x_m)\\).\nNow we want prove that \\(\\bar x\\) is the stationary point of \\(f\\).\nFrom (*), we know that\n\\[f(z_1^{k_j})\\leq f(x_1,x_2^{k_j},..., x_m^{k_j})\\qquad \\forall x_1\\in X_1\\]\nLet \\(j\\rightarrow +\\infty\\), we derive\n\\[f(\\bar x)\\leq f(x_1,\\bar x_2,..., \\bar x_m)\\overset \\Delta = h(x_1)\\qquad \\forall x_1\\in X_1\\]\nwhich implies that \\(\\bar x_i\\) is the minima of \\(h(x_1)\\) on \\(X_1\\). Using the optimality over a convex set, we conclude that\n\\[h\u0026#39;(\\bar x_1)(\\bar x_1 -x_1)\\geq 0 \\Leftrightarrow (x_1-\\bar x_1)^T\\nabla_1f(\\bar x_1)\\geq 0\\qquad x_1\\in X_1\\]\nAt this stage, if we can prove that \\(\\{z_1^{k_j}\\}\\) converges to \\(\\bar x\\), we can show that\n\\[ (x_2-\\bar x_2)^T\\nabla_2 f(\\bar x_2)\\geq 0\\qquad x_2\\in X_2\\], since\n\\[f(z_1^{k_j})=f(x_1^{k_j+1},x_2^{k_j},x_3^{k_j},...,x_m^{k_j})\\leq f(x_1^{k_j+1},x_2,x_3^{k_j},...,x_m^{k_j})\\qquad x_2\\in X_2\\]\nLet \\(j\\rightarrow +\\infty\\), we derive\n\\[f(\\bar x)\\leq f(\\bar x_1,\\bar x_2,\\bar x_3,..., \\bar x_m)\\qquad \\forall x_2\\in X_2\\]\nand\n\\[(x_2-\\bar x_2)^T\\nabla_2f(\\bar x_2)\\geq 0\\qquad x_2\\in X_2\\]\n(Note: Although \\(x_1^{k_j+1}\\) may not in the sequence \\(\\{x_1^{k_t}\\}_{t\\geq 1}\\) ,which convergences to \\(\\bar x_1\\), but \\(\\{z_1^{k_j}\\}\\) converges to \\(\\bar x\\), so its component \\(x_1^{k_j+1}\\) converges to \\(\\bar x_1\\)).\nFurthermore, if we prove that for \\(i=1,2,...,m-1\\),\\(\\{z_i^{k_j}\\}\\) convergences to \\(\\bar x\\), then we have\n\\[(x_i-\\bar x_i)^T\\nabla_i\\;f(\\bar x_i)\\geq 0\\qquad x_i\\in X_i\\]\nAnd thus \\(\\bar x\\) is a stationary point, since \\((x-\\bar x)^T\\nabla f(\\bar x)\\geq 0\\)\nBy far, it remains to prove that \\(\\{z_i^{k_j}\\}\\quad,\\forall i\\) convergence to \\(\\bar x\\). First,we try to prove that \\(\\{z_1^{k_1}\\}\\) convergence to \\(\\bar x\\).\nAssume the contrary that \\(r^{k_j}=\\vert \\vert z_1^{k_j}-x^{k_j}\\vert \\vert\\) doesn't convergence to 0. Let \\(s_1^{k_j}=(z_1^{k_j}-x^{k_j})/r^{k_j}\\). Thus, \\(z_1^{k_j}=x^{k_j}+r^{k_j}s_1^{k_j}\\) , \\(\\vert \\vert r_{k_j}\\vert \\vert =1\\) and \\(s_1^{k_j}\\) differs from 0 only along the first block-component. Since \\(\\{s_1^{k_j}\\}\\) belong to a compact set and therefore without loss of generality, we assume \\(s_1^{k_j}\\) convergences to \\(\\bar s_1\\).\nSince \\(r^{k_j}\u0026gt;0\\),we can find a \\(\\epsilon\\in (0,1)\\), such that \\(x^{k_j}+\\epsilon s_1^{k_j}\\) lies on the segment joining \\(x^{k_j}\\) and \\(x^{k_j}+s_1^{k_j}=z_1^{k_j}\\). Using the non-increasing property of \\(f\\),we derive,\n\\[f(z_1^{k_j})\\leq f(x^{k_j}+\\epsilon s_1^{k_j}) \\leq f(x^{k_j})\\]\nAgain, using (*), we conclude\n\\[f(x^{k_{j+1}})\\leq f(z_1^{k_j})\\leq f(x^{k_j}+\\epsilon s_1^{k_j}) \\leq f(x^{k_j})\\]\nLet \\(j\\rightarrow +\\infty\\), we derive \\(f(\\bar x)=f(\\bar x+\\epsilon \\bar s_1)\\), which contradicts the hypothesis that \\(f\\) is uniquely minimized when viewed as a function of the first block component. This contradiction establishes that \\(\\{z_1^{k_1}\\}\\) convergence to \\(\\bar x\\).\nSimilarly, let \\(r_t^{k_j}=\\vert \\vert z_t^{k_j}-z_{t-1}^{k_j}\\vert \\vert\\) for \\(t=2,3,...,m-1\\) and using the same technique shown above, we finally prove that \\(\\{z_i^{k_j}\\},\\quad \\forall i\\).\n Powell's example In ON SEARCH DIRECTIONS FOR MINIMIZATION ALGORITHMS, Power actually gives three examples that sequences generated by the algorithm discussed above do not convergence to stationary points once some hypothesis are not met.\n The first example is straightforward, However, the remarkable properties of this example can be destroyed by making a small perturbation to the starting vector \\(x^0\\).\n The second example is not sensitive to either small changes in the initial data or to small errors introduced during the iterative process, for example computer rounding errors.\n The third example suggests that a function that is infinitely differentiable that also causes an endless loop in the iterative minimization method.\n  We here only presents the first example. Consider the following function\n\\[f(x,y,z)=-(xy+yz+zx)+(x-1)_+^2+(-x-1)_+^2+(y-1)_+^2+(-y-1)_+^2+(z-1)_+^2+(-z-1)_+^2\\]\nwhere\n\\[(x-c)_+^2=\\begin{cases}0,x-c\u0026lt; 0\\\\ (x-c)^2,x-c\\geq 0\\end{cases}\\]\nGiven the starting point \\(x_0=(-1-e,1+\\frac{1}{2}e,-1-\\frac{1}{4}e)\\) and use block coordinate decent algorithm,and we update the variable in a manner of \\(x\\rightarrow y\\rightarrow z\\rightarrow x ...\\) with\n\\[x_{k+1}^{**}\\leftarrow \\text{sign}(y_k+z_k)[1+\\frac{1}{2}\\vert y_k+z_k\\vert ]\\]\n\\[y_{k+1}^{**}\\leftarrow \\text{sign}(x_{k+1}+z_k)[1+\\frac{1}{2}\\vert x_{k+1}+z_k\\vert ]\\]\n\\[z_{k+1}^{**}\\leftarrow \\text{sign}(x_{k+1}+y_{k+1})[1+\\frac{1}{2}\\vert x_{k+1}+y_{k+1}\\vert ]\\]\nWe here present the first six steps of this case\n  cycle/totall iteration x y z    1/1 1+\\(\\frac{1}{8}e\\) 1+$e $ -1-\\(\\frac{1}{4}e\\)  1/2 1+\\(\\frac{1}{8}e\\) -1-\\(\\frac{1}{16}e\\) -1-\\(\\frac{1}{4}e\\)  1/3 1+\\(\\frac{1}{8}e\\) -1-\\(\\frac{1}{16}e\\) 1+\\(\\frac{1}{32}e\\)  2/4 -1-\\(\\frac{1}{64}e\\) -1-\\(\\frac{1}{16}e\\) 1+\\(\\frac{1}{32}e\\)  2/5 -1-\\(\\frac{1}{64}e\\) 1+\\(\\frac{1}{128}e\\) 1+\\(\\frac{1}{32}e\\)  2/6 -1-\\(\\frac{1}{64}e\\) 1+\\(\\frac{1}{128}e\\) -1-\\(\\frac{1}{256}e\\)  3/7 1+\\(\\frac{1}{512}e\\) 1+\\(\\frac{1}{128}e\\) -1-\\(\\frac{1}{256}e\\)  ... ... ... ...    This result implies that the sequence obtained by this algorithm can not converge to one single point since \\(x-coordinate\\) change its sign as the even cycle and odd cycle alternate. Situations are similar for \\(y-coordinate\\) and \\(z-coordinate\\).\nBut \\(\\{x_k\\}\\) has six sub-sequences which convergence to (1,1,-1), (1,-1,-1), (1,-1,1), (-1,-1,1),(-1,-1,1),(-1,1,1),(-1,1,-1) respectively.\n Remark\nA hint to derive the update formula:\n\\[x\\leftarrow \\text{sign}(y+z)[1+\\frac{1}{2}(y+z)]\\]\nIndeed, derivates of \\((x-1)_+^2\\) and \\((-x-1)_+^2\\) are as follows respecively\n\\[\\frac{d(x-1)_+^2}{dx}=\\begin{cases}2(x-1),x\\geq 1\\\\0,x\u0026lt;1\\end{cases}\\quad \\frac{d(-x-1)_+^2}{dx}=\\begin{cases}2(-x-1),x\\leq -1\\\\0,x\u0026gt;-1\\end{cases} \\]\nSo for the univariate optimization problem, setting the derivate of \\(g(x)=f(x,y,z)\\) to zero, we conclude\n\\[\\frac{\\partial f(x,y,x)}{\\partial x}=0\\Rightarrow \\begin{cases}x\\geq 1: x=1+\\frac{1}{2}(y+z)\\\\-1\u0026lt; x\u0026lt;1: -(y+z)=0\\\\x\\leq -1:x=-1+\\frac{1}{2}(y+z) \\end{cases}\\]\n The gradient of \\(f(x,y,z)\\) on this cyclic path, is \\(\\nabla f(x,y,z)=(-y-z,-x-z,-x-y)\\) and \\(\\vert \\vert \\nabla f(x,y,z)\\vert \\vert _1=2\\)\n This example is unstable with respect to small perturbations. Small changes in the starting point \\(x_0=(-1-e,1+\\frac{1}{2}e,-1-\\frac{1}{4}e)\\) or smal errors in the numbers that are computed during the calculation will destroy the cyclic behavior.\nIt's s clear the choice of perturbations \\(e\\) plays a key role. Say, \\(x_0=(-1-e_1,1+e_2,-1-e_3)\\) and we have \\(e_k=\\frac{1}{2}(e_{k-2}- e_{k-1})\\)\n  cycle/totall iteration x y z    1/1 1+\\(e_4\\) 1+\\(e_2\\) -1-\\(e_3\\)  1/2 1+\\(e_4\\) -1-\\(e_5\\) -1-\\(e_3\\)  1/3 1+\\(e_4\\) -1-\\(e_5\\) 1+\\(e_6\\)  2/4 -1-\\(e_7\\) -1-\\(e_5\\) 1+\\(e_6\\)  2/5 -1-\\(e_7\\) 1+\\(e_8\\) 1+\\(e_6\\)  2/6 -1-\\(e_7\\) 1+\\(e_8\\) -1-\\(e_9\\)  ... ... ... ...    To preserve the cyclic behavior , we have to make sure that \\(e_{k-2}\u0026gt;e_{k-1}\\)\nAnd in practice, when we do some numerical tests, we shall find that, this theoretically-existed endless loop actual breaks down due to the rounding errors. A brief illustration is given below. In this experiment, loop ends at the 52 steps.\n As\n\\[\\frac{\\partial f(x,y,x)}{\\partial x}=0\\Rightarrow \\begin{cases}x\\geq 1: x=1+\\frac{1}{2}(y+z)\\\\-1\u0026lt; x\u0026lt;1: -(y+z)=0\\\\x\\leq -1:x=-1+\\frac{1}{2}(y+z) \\end{cases}\\]\nsuggests that, when \\(-1\u0026lt;x\u0026lt;1\\), the choice of \\(x\\) is arbitrary and we set \\(x^*=0\\) in the case above. So the uniqueness requirement is violated. It turns out that the six vertices are even not the stationary points.\nFor example, at point \\(\\bar x=(1,1,-1)\\), \\(\\nabla f(\\bar x)=(0,0,-2)\\) and for any ponit \\(x\\) in the unit cubic \\((x-\\bar x)^T\\nabla f(\\bar x)\\leq 0\\). Say, \\(x=(0.9,0.9,-0.9)\\), \\((x-\\bar x)^T\\nabla f(\\bar x)=-0.2\u0026lt;0\\)\nActually, as in the proof of Theorem, we prove that \\(\\{z_1^{k_j}\\}\\) converges to \\(\\bar x\\), where \\(\\bar x\\) is the limit point of \\(\\{x^{k_j}\\}\\). But in this example, the limit point of \\(\\{z_1^{k_j}\\}\\) is (1,1,-1) while the limit point of \\(\\{x^{k_j}\\}\\) is either (-1,1,-1) or (1,-1,1). So the requirement of uniqueness is not met.\n   R codes for numerical experiments #################### ### Function for test ### #################### PowellE1\u0026lt;-function(xstart,cycles,fig=T){ #######function part ############## UpdateCycle\u0026lt;-function(x){ Sign\u0026lt;-function(x){ if (x\u0026gt;0){ return(1) }else{ if (x\u0026lt;0){ return(-1) }else{ return(0) } } } x.new\u0026lt;-c() x.new[1]\u0026lt;-Sign(x[2]+x[3])*(1+0.5*abs(x[2]+x[3])) x.new[2]\u0026lt;-Sign(x.new[1]+x[3])*(1+0.5*abs(x.new[1]+x[3])) x.new[3]\u0026lt;-Sign(x.new[1]+x.new[2])*(1+0.5*abs(x.new[1]+x.new[2])) cycle\u0026lt;-matrix(c(x.new[1],x[2],x[3],x.new[1],x.new[2],x[3],x.new[1],x.new[2],x.new[3]), ncol=3,byrow=T) return(cycle) } fpowell\u0026lt;-function(x){ PostivePart\u0026lt;-function(x){ ifelse(x\u0026gt;=0,x,0) } fval\u0026lt;-(-(x[1]*x[2]+x[2]*x[3]+x[1]*x[3]))+ PostivePart(x[1]-1)^2+PostivePart(-x[1]-1)^2+ PostivePart(x[2]-1)^2+PostivePart(-x[2]-1)^2+ PostivePart(x[3]-1)^2+PostivePart(-x[3]-1)^2 return(fval) } ############ operation part ################ x.store\u0026lt;-matrix(ncol=3,nrow=cycles*3+1) x.store[1,]\u0026lt;-xstart for (i in seq_len(cycles)){ x.store[(3*i-1):(3*i+1),]\u0026lt;-UpdateCycle(x.store[3*i-2,]) } x.store\u0026lt;-x.store[-1,] fval\u0026lt;-rep(0,cycles*3) for(i in seq_len(cycles*3)){ fval[i]\u0026lt;-fpowell(x.store[i,]) } fval\u0026lt;-as.matrix(fval) if (fig==T){ plot(fval,ylim=c(min(fval)-1,max(fval)+1),type=\u0026quot;l\u0026quot;,xlab=\u0026quot;Iterations\u0026quot;,ylab = \u0026quot;F value\u0026quot;) } r\u0026lt;-list() r$x.iterate\u0026lt;-x.store r$fval\u0026lt;-fval return(r) } ################## #### Test 1 ######## ################## perturb\u0026lt;-0.5 xstart\u0026lt;-c(-1-perturb,1+0.5*perturb,-1-0.25*perturb) cycles\u0026lt;-20 r\u0026lt;-PowellE1(xstart,cycles,fig=T) ################## #### Test 2 ######## ################## perturb\u0026lt;-0.5 xstart\u0026lt;-c(-1-perturb,1+0.5*perturb,-1-0.25*perturb) cycles\u0026lt;-20 r\u0026lt;-PowellE1(xstart,cycles,fig=T) ################## #### Test 3 ######## ################## xstart\u0026lt;-c(3,2,1) cycles\u0026lt;-100 r\u0026lt;-PowellE1(xstart,cycles,fig=T)  ","date":1479340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596511776,"objectID":"8412461298db1d9651b4bc3fba2d8fb8","permalink":"/post/convergence-analysis-for-block-coordinate-descent-algorithm-and-powells-examples/","publishdate":"2016-11-17T00:00:00Z","relpermalink":"/post/convergence-analysis-for-block-coordinate-descent-algorithm-and-powells-examples/","section":"post","summary":"Convergence analysis of Block coordinate decent with exact minimization.","tags":["Powell-Example"],"title":"Convergence Analysis for Block Coordinate Decent Algorithm and Powell's Examples","type":"post"}]