---
title: Momentum - Last Iterate Convergence and Variance Reduction
event: OptML@Lehigh 2023 Spring
# event_url: https://www.abstractsonline.com/pp8/#!/10693/presentation/6549
location: Bethlehem, PA, United States
summary: Understand the advantage of SGD with moment over vanilla SGD.
abstract: "Stochastic gradient descent with momentum (SGD + M) is very popular, yet its convergence property is not well understood. Only recently, the work a proved that stochastic heavy-ballâ€™s (SGD + H) convergence speed is not faster than that of SGD alone (convex case). In this talk, we review some literature on this topic and results. It seems that the advantage of SGD + M over SGD in convex case is to improve the last iterate convergence."

# Talk start and end times.
#   End time can optionally be hidden by prefixing the line with `#`.
date: "2023-02-11T18:15:00Z"
date_end: "2022-02-11T20:15:00Z"
all_day: false

# Schedule page publish date (NOT talk date).
publishDate: "2017-01-01T00:00:00Z"

authors: []
tags: []

# Is this a featured talk? (true/false)
featured: true

# image:
#   caption: 'Image credit: [**Unsplash**](https://unsplash.com/photos/bzdhc5b3Bxs)'
#   focal_point: Right

# links:
# - icon: twitter
#   icon_pack: fab
#   name: Follow
#   url: https://twitter.com/georgecushen
# url_code: ""
url_pdf: ""
url_slides: "main.pdf"
url_video: ""

# Markdown Slides (optional).
#   Associate this talk with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
# slides: example

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
# projects:
# - internal-project

# Enable math on this page?
math: true
---

