---
title: Anomaly Detection
author: Yutong
date: '2019-01-04'
slug: anomaly-detection
categories:
  - Statistics
tags:
  - data-analysis
image:
  caption: ''
  focal_point: ''
---



<div id="point-anomaly-detection---grubbs-test" class="section level1">
<h1>Point Anomaly Detection - Grubbs' test</h1>
<p>Grubbs' test<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> is commonly used technique to detect an outlier in <strong>univariate</strong> problem, where <strong>normality</strong> assumption is required. It can be formualted as either one-side testing problem or two-sided testing problem.</p>
<p>The hypothesis test is defined as</p>
<p><span class="math display">\[H_0: \text{There are no outlier in the data set} \quad H_1: \text{There is exactly one outlier in the data set}.\]</span> For two-sided testing, it tries to determine whether the observation with the largest absolute deviation is an outlier, where the test statistic is defined as</p>
<p><span class="math display">\[
G = \frac{\max_i |X_i - \bar X|}{s},
\]</span> where the <span class="math inline">\(\bar X\)</span> is the sample mean and <span class="math inline">\(s\)</span> is the sample deviation.</p>
<p>Let's look at one simulated example.</p>
<pre class="r"><code>set.seed(123)
simulated_data &lt;- rnorm(100, 0, 1)
simulated_data_with_outliers &lt;- c(simulated_data, c(3.5, -3.7))
# normality check
shapiro.test(simulated_data_with_outliers)</code></pre>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  simulated_data_with_outliers
## W = 0.9804, p-value = 0.1344</code></pre>
<p>Shapiro-Wilk normality test impiles the data is normally distributed. Now, let's performe the grubbs' test.</p>
<pre class="r"><code>library(outliers)
grubbs.test(simulated_data_with_outliers, two.sided = TRUE)</code></pre>
<pre><code>## 
##  Grubbs test for one outlier
## 
## data:  simulated_data_with_outliers
## G = 3.65380, U = 0.86651, p-value = 0.01626
## alternative hypothesis: lowest value -3.7 is an outlier</code></pre>
<p>The test result detecs the lowest value as an outlier.</p>
<p>Let's remove the -3.7 and performe the test again.</p>
<pre class="r"><code>grubbs.test(head(simulated_data_with_outliers,101), two.sided = TRUE)</code></pre>
<pre><code>## 
##  Grubbs test for one outlier
## 
## data:  head(simulated_data_with_outliers, 101)
## G = 3.48190, U = 0.87755, p-value = 0.03378
## alternative hypothesis: highest value 3.5 is an outlier</code></pre>
<p>Finally, let's remove two outliers altogether.</p>
<pre class="r"><code>grubbs.test(head(simulated_data_with_outliers,100), two.sided = TRUE)</code></pre>
<pre><code>## 
##  Grubbs test for one outlier
## 
## data:  head(simulated_data_with_outliers, 100)
## G = 2.62880, U = 0.92949, p-value = 0.7584
## alternative hypothesis: lowest value -2.30916887564081 is an outlier</code></pre>
<p>This impiles there are no outliers.</p>
<p>Grubbs' test is useful for identify the outliers of a small amount one at a time, but not suitable to detect a group of outliers.</p>
</div>
<div id="collective-anomaly-detection" class="section level1">
<h1>Collective Anomaly Detection</h1>
<div id="anomaly-in-timeseries---seasonal-hybrid-esd-algorithm" class="section level2">
<h2>Anomaly in timeseries - Seasonal Hybrid ESD algorithm</h2>
<pre class="r"><code>#  devtools::install_github(&quot;twitter/AnomalyDetection&quot;)
library(AnomalyDetection)
river &lt;- read.csv(&quot;https://raw.githubusercontent.com/Rothdyt/personal-blog/master/static/post/dataset/river.csv&quot;)
results &lt;- AnomalyDetectionVec(river$nitrate, period=12, direction = &#39;both&#39;, plot = T)</code></pre>
<pre class="r"><code>results$plot</code></pre>
<p><img src="/post/anomaly-detection/index_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
<div id="distance-based-anomaly-detection" class="section level2">
<h2>Distance-based Anomaly Detection</h2>
<div id="global-anomaly---largest-distance" class="section level3">
<h3>Global Anomaly - Largest Distance</h3>
<p>Intuitively, the larger distance the more likely the point would be an outlier.</p>
<pre class="r"><code>library(FNN)</code></pre>
<pre><code>## Warning: package &#39;FNN&#39; was built under R version 3.4.4</code></pre>
<pre class="r"><code>furniture &lt;- read.csv(&quot;https://raw.githubusercontent.com/Rothdyt/personal-blog/master/static/post/dataset/furniture.csv&quot;)
furniture_scaled &lt;- data.frame(Height = scale(furniture$Height), Width = scale(furniture$Width))
furniture_knn &lt;- get.knn(furniture_scaled, k = 5)
furniture_scaled$score_knn &lt;- rowMeans(furniture_knn$nn.dist)
largest_idx &lt;- which.max(furniture_scaled$score_knn)
plot(furniture_scaled$Height, furniture_scaled$Width, cex=sqrt(furniture_scaled$score_knn), pch=20)
points(furniture_scaled$Height[largest_idx], furniture_scaled$Width[largest_idx], col=&quot;red&quot;, pch=20)</code></pre>
<p><img src="/post/anomaly-detection/index_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
</div>
<div id="local-anomaly---lof" class="section level3">
<h3>Local Anomaly - LOF</h3>
<p>kNN is useful for finding global anomalies, but is less able to surface local outliers.</p>
<p>LOF is a ratio of densities:</p>
<ul>
<li>LOF &gt; 1 more likely to be anomalous LOF ≤ 1 less likely to be anomalous</li>
<li>Large LOF values indicate more isolated points</li>
</ul>
<pre class="r"><code>library(dbscan)
furniture_lof &lt;- furniture[,2:3]
furniture_lof$score_lof &lt;- lof(scale(furniture_lof), k=5)
largest_idx &lt;- which.max(furniture_lof$score_lof)
plot(furniture_lof$Height, furniture_lof$Width, cex=sqrt(furniture_lof$score_lof), pch=20)
points(furniture_lof$Height[largest_idx], furniture_lof$Width[largest_idx], col=&quot;red&quot;, pch=20)</code></pre>
<p><img src="/post/anomaly-detection/index_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>It's clear that, lof successfuly detects the local outlier.</p>
</div>
</div>
</div>
<div id="isolation-forest" class="section level1">
<h1>Isolation Forest</h1>
<ul>
<li>Isolation Forest is built on the basis of decision trees;</li>
<li>To grow a decision tree, at each node, a feature and a corresponding cutoff value are randomly selected;</li>
<li>Intuitively, outliers are less frequent than regular observations and are different from them in terms of values, so outliers should be identified closer to the root of the tree with fewer splits. We use isolation score to characterize this.</li>
</ul>
<div id="isolation-score" class="section level2">
<h2>Isolation Score</h2>
<p>We need some quatity to define the isolation score<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
<ul>
<li><strong>Path Length</strong>: <span class="math inline">\(h(x)\)</span> of a point <span class="math inline">\(x\)</span> is measured by the number of edges <span class="math inline">\(x\)</span> traverses an iTree from the root node until the traversal is terminated at an external node.</li>
<li><strong>Normalizing constant</strong> <span class="math display">\[c(n) = 2H(n − 1) − (2(n − 1)/n)\]</span>, where <span class="math inline">\(n\)</span> is the number of samples to grow a tree and <span class="math inline">\(H(i)\)</span> is the harmonic number and it can be estimated by <span class="math inline">\(ln(i) + 0.5772156649\)</span> (Euler’s constant).</li>
</ul>
<p>The isolation score <span class="math inline">\(s\)</span> of an sample <span class="math inline">\(x\)</span> is defined as <span class="math display">\[s(x,n)= 2^{-\frac{E(h(x))}{c(n)}},\]</span> where the <span class="math inline">\(E()\)</span> is the expectation of <span class="math inline">\(h(x)\)</span>.</p>
<ul>
<li><p>Interpreting the isolation score:</p></li>
<li>Scores between 0 and 1</li>
<li><p>Scores near 1 indicate anomalies (small path length)</p></li>
</ul>
<pre class="r"><code># devtools::install_github(&quot;Zelazny7/isofor&quot;)
library(isofor)
furniture &lt;- read.csv(&quot;https://raw.githubusercontent.com/Rothdyt/personal-blog/master/static/post/dataset/furniture.csv&quot;)
furniture &lt;- data.frame(Height = furniture$Height, Width = furniture$Width)
scores &lt;- matrix(nrow=dim(furniture)[1])
for (ntree in c(100, 200, 500)){
  furniture_tree &lt;- iForest(furniture, nt = ntree, phi=50)
  scores &lt;- cbind(scores, predict(furniture_tree, furniture)) 
}
plot(scores[,3], scores[,4], xlab = &quot;200 tress&quot;, ylab=&quot;500 tress&quot;)
abline(a=0,b=1)</code></pre>
<p><img src="/post/anomaly-detection/index_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>This graph is used to assess wheter the number of trees is enough for the isolation score to converge. From the graph above, we know that 200 tress are enough for us to identify the anomalies.</p>
<pre class="r"><code>library(lattice)
furniture_forest &lt;- iForest(furniture, nt = 200, phi=50)
h_seq &lt;- seq(min(furniture$Height), max(furniture$Height), length.out = 20) 
w_seq &lt;- seq(min(furniture$Width), max(furniture$Width), length.out = 20)
furniture_grid &lt;- expand.grid(Width = w_seq, Height = h_seq)
furniture_grid$score &lt;- predict(furniture_forest, furniture_grid)
contourplot(score ~ Height + Width, data = furniture_grid,region = TRUE)</code></pre>
<p><img src="/post/anomaly-detection/index_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>This contour graph used to identify the anomaly regions.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p><a href="https://en.wikipedia.org/wiki/Grubbs%27_test_for_outliers">https://en.wikipedia.org/wiki/Grubbs%27_test_for_outliers</a><a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Zhihua Zhou et al. <a href="https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf" class="uri">https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf</a><a href="#fnref2">↩</a></p></li>
</ol>
</div>
