---
title: Convergence Analysis for Block Coordinate Decent Algorithm and Powell's Examples
author: Yutong Dai
date: '2016-11-17'
slug: convergence-analysis-for-block-coordinate-descent-algorithm-and-powell's-examples
categories:
  - optimization
tags:
  - Powell-Example
output:
  blogdown::html_page:
    toc: yes
summary: "Convergence analysis of Block coordinate decent with exact minimization."
draft: true
---


<div id="TOC">
<ul>
<li><a href="#problem-description">Problem description</a><ul>
<li><a href="#notations">Notations</a></li>
<li><a href="#assumption">Assumption</a></li>
<li><a href="#algorithm">Algorithm</a></li>
</ul></li>
<li><a href="#convergence-analysis">Convergence Analysis</a></li>
<li><a href="#powells-example">Powell's example</a></li>
<li><a href="#r-codes-for-numerical-experiments">R codes for numerical experiments</a></li>
</ul>
</div>

<p>We mainly focus on the convergence of Block coordinate decent with exact minimization, whose block update strategy employs Gauss-Seidel manner. And then use Powell's example to see what will happen if some conditions are not met.</p>
<blockquote>
<p>Reference: 1. Dimitri .P Bertsekas, Nonlinear Programming 2ed 2. Powell ,1973, ON SEARCH DIRECTIONS FOR MINIMIZATION ALGORITHMS</p>
</blockquote>
<div id="problem-description" class="section level1">
<h1>Problem description</h1>
<div id="notations" class="section level2">
<h2>Notations</h2>
<p>We want to solve the problem:</p>
<p><span class="math display">\[\mathop{min}_{x\in X}\quad f(x)\]</span></p>
<p>where X is a Cartesian product of closed convex sets $X_1,...,X_m:X=_{i=1}^n X_i $</p>
<p>We assume that <span class="math inline">\(X_i\)</span> is a closed convex subset of <span class="math inline">\(R^{n_i}\)</span> and <span class="math inline">\(n=\sum_{i=1}^m n_i\)</span>. The vector is partitioned into <span class="math inline">\(m\)</span> block(s) such that <span class="math inline">\(x_i \in X^{n_i}\)</span>.</p>
<p>We denote <span class="math inline">\(\nabla_i f\)</span> as the gradient of <span class="math inline">\(f\)</span> with respect to component <span class="math inline">\(x_i\)</span>.</p>
</div>
<div id="assumption" class="section level2">
<h2>Assumption</h2>
<p>We shall assume that for every <span class="math inline">\(x\in X\)</span> and <span class="math inline">\(i=1,2,...m\)</span> the optimization problem</p>
<p><span class="math display">\[\mathop{min}_{\xi\in X_i}\quad f(x_1,...,x_{i-1},\xi,x_{i+1,....,x_m})\]</span></p>
<p>has <strong>at least one solution</strong>.</p>
</div>
<div id="algorithm" class="section level2">
<h2>Algorithm</h2>
<p>The Gauss-Seidel method, generates the next iterate <span class="math inline">\(x^{k+1}=(x^{k+1}_1,...,x^{k+1}_m)\)</span>, given the current the iterate <span class="math inline">\(x^{k}=(x^{k}_1,...,x^{k}_m)\)</span>, according to the iteration</p>
<p><span class="math display">\[x^{k+1}_i=\mathop{argmin}_{\xi\in X_i}\quad f(x_1^{k+1},...,x^{k+1}_{i-1},\xi,x^k_{i+1},...,x_m^k)\]</span></p>
</div>
</div>
<div id="convergence-analysis" class="section level1">
<h1>Convergence Analysis</h1>
<p><code>Theorem</code> Suppose that <span class="math inline">\(f\)</span> is <strong>continuously differentiable</strong> over the set <span class="math inline">\(X\)</span> defined as above. Furthermore, suppose that for each <span class="math inline">\(i\)</span> and <span class="math inline">\(x\in X\)</span>,</p>
<p><span class="math display">\[f(x_1,...,x_{i-1},\xi,x_{i+1,....,x_m})\]</span></p>
<p>viewed as a function of <span class="math inline">\(\xi\)</span>, attains a unique minimum <span class="math inline">\(\bar x_i\)</span> over <span class="math inline">\(X_i\)</span> and is monotonically non-increasing in the interval from <span class="math inline">\(x_i\)</span> to <span class="math inline">\(\bar \xi\)</span>. Let <span class="math inline">\(\{x_k\}\)</span> be the sequence generated by the block coordinate method with Gauss-Seidel manner. Then, every limit point of <span class="math inline">\(\{x_k\}\)</span> is a stationary point.</p>
<p><code>PROOF</code></p>
<p>Let</p>
<p><span class="math display">\[z_i^k=(x_1^{k+1},...,x_i^{k+1},x_{i+1}^k,...,x_m^k)\]</span></p>
<p>By the nature of this algorithm, for all <span class="math inline">\(k\geq 0\)</span>, we have following inequality</p>
<p><span class="math display">\[f(x^k)\geq f(z_1^k)\geq f(z_2^k)\geq ...\geq f(z_{m-1}^k)\geq f(x^{k+1}) \quad (*)\]</span></p>
<p>Since <span class="math inline">\(\{x_k\}in X\)</span>, we can assume <span class="math inline">\(\{x^{k_j}\}\)</span> is the subsequence that converges to <span class="math inline">\(\bar x=(\bar x_1,..,\bar x_m)\)</span>.</p>
<p>Now we want prove that <span class="math inline">\(\bar x\)</span> is the stationary point of <span class="math inline">\(f\)</span>.</p>
<p>From (*), we know that</p>
<p><span class="math display">\[f(z_1^{k_j})\leq f(x_1,x_2^{k_j},..., x_m^{k_j})\qquad \forall x_1\in X_1\]</span></p>
<p>Let <span class="math inline">\(j\rightarrow +\infty\)</span>, we derive</p>
<p><span class="math display">\[f(\bar x)\leq f(x_1,\bar x_2,..., \bar x_m)\overset \Delta = h(x_1)\qquad \forall x_1\in X_1\]</span></p>
<p>which implies that <span class="math inline">\(\bar x_i\)</span> is the minima of <span class="math inline">\(h(x_1)\)</span> on <span class="math inline">\(X_1\)</span>. Using the optimality over a convex set, we conclude that</p>
<p><span class="math display">\[h&#39;(\bar x_1)(\bar x_1 -x_1)\geq 0 \Leftrightarrow (x_1-\bar x_1)^T\nabla_1f(\bar x_1)\geq 0\qquad x_1\in X_1\]</span></p>
<p>At this stage, if we can prove that <span class="math inline">\(\{z_1^{k_j}\}\)</span> converges to <span class="math inline">\(\bar x\)</span>, we can show that</p>
<p><span class="math display">\[ (x_2-\bar x_2)^T\nabla_2 f(\bar x_2)\geq 0\qquad x_2\in X_2\]</span>, since</p>
<p><span class="math display">\[f(z_1^{k_j})=f(x_1^{k_j+1},x_2^{k_j},x_3^{k_j},...,x_m^{k_j})\leq f(x_1^{k_j+1},x_2,x_3^{k_j},...,x_m^{k_j})\qquad x_2\in X_2\]</span></p>
<p>Let <span class="math inline">\(j\rightarrow +\infty\)</span>, we derive</p>
<p><span class="math display">\[f(\bar x)\leq f(\bar x_1,\bar x_2,\bar x_3,..., \bar x_m)\qquad \forall x_2\in X_2\]</span></p>
<p>and</p>
<p><span class="math display">\[(x_2-\bar x_2)^T\nabla_2f(\bar x_2)\geq 0\qquad x_2\in X_2\]</span></p>
<p>(Note: Although <span class="math inline">\(x_1^{k_j+1}\)</span> may not in the sequence <span class="math inline">\(\{x_1^{k_t}\}_{t\geq 1}\)</span> ,which convergences to <span class="math inline">\(\bar x_1\)</span>, but <span class="math inline">\(\{z_1^{k_j}\}\)</span> converges to <span class="math inline">\(\bar x\)</span>, so its component <span class="math inline">\(x_1^{k_j+1}\)</span> converges to <span class="math inline">\(\bar x_1\)</span>).</p>
<p>Furthermore, if we prove that for <span class="math inline">\(i=1,2,...,m-1\)</span>,<span class="math inline">\(\{z_i^{k_j}\}\)</span> convergences to <span class="math inline">\(\bar x\)</span>, then we have</p>
<p><span class="math display">\[(x_i-\bar x_i)^T\nabla_i\;f(\bar x_i)\geq 0\qquad x_i\in X_i\]</span></p>
<p>And thus <span class="math inline">\(\bar x\)</span> is a stationary point, since <span class="math inline">\((x-\bar x)^T\nabla f(\bar x)\geq 0\)</span></p>
<p>By far, it remains to prove that <span class="math inline">\(\{z_i^{k_j}\}\quad,\forall i\)</span> convergence to <span class="math inline">\(\bar x\)</span>. First,we try to prove that <span class="math inline">\(\{z_1^{k_1}\}\)</span> convergence to <span class="math inline">\(\bar x\)</span>.</p>
<p>Assume the contrary that <span class="math inline">\(r^{k_j}=\vert \vert z_1^{k_j}-x^{k_j}\vert \vert\)</span> doesn't convergence to 0. Let <span class="math inline">\(s_1^{k_j}=(z_1^{k_j}-x^{k_j})/r^{k_j}\)</span>. Thus, <span class="math inline">\(z_1^{k_j}=x^{k_j}+r^{k_j}s_1^{k_j}\)</span> , <span class="math inline">\(\vert \vert r_{k_j}\vert \vert =1\)</span> and <span class="math inline">\(s_1^{k_j}\)</span> differs from 0 only along the first block-component. Since <span class="math inline">\(\{s_1^{k_j}\}\)</span> belong to a compact set and therefore without loss of generality, we assume <span class="math inline">\(s_1^{k_j}\)</span> convergences to <span class="math inline">\(\bar s_1\)</span>.</p>
<p>Since <span class="math inline">\(r^{k_j}&gt;0\)</span>,we can find a <span class="math inline">\(\epsilon\in (0,1)\)</span>, such that <span class="math inline">\(x^{k_j}+\epsilon s_1^{k_j}\)</span> lies on the segment joining <span class="math inline">\(x^{k_j}\)</span> and <span class="math inline">\(x^{k_j}+s_1^{k_j}=z_1^{k_j}\)</span>. Using the non-increasing property of <span class="math inline">\(f\)</span>,we derive,</p>
<p><span class="math display">\[f(z_1^{k_j})\leq f(x^{k_j}+\epsilon s_1^{k_j}) \leq f(x^{k_j})\]</span></p>
<p>Again, using (*), we conclude</p>
<p><span class="math display">\[f(x^{k_{j+1}})\leq f(z_1^{k_j})\leq f(x^{k_j}+\epsilon s_1^{k_j}) \leq f(x^{k_j})\]</span></p>
<p>Let <span class="math inline">\(j\rightarrow +\infty\)</span>, we derive <span class="math inline">\(f(\bar x)=f(\bar x+\epsilon \bar s_1)\)</span>, which contradicts the hypothesis that <span class="math inline">\(f\)</span> is uniquely minimized when viewed as a function of the first block component. This contradiction establishes that <span class="math inline">\(\{z_1^{k_1}\}\)</span> convergence to <span class="math inline">\(\bar x\)</span>.</p>
<p>Similarly, let <span class="math inline">\(r_t^{k_j}=\vert \vert z_t^{k_j}-z_{t-1}^{k_j}\vert \vert\)</span> for <span class="math inline">\(t=2,3,...,m-1\)</span> and using the same technique shown above, we finally prove that <span class="math inline">\(\{z_i^{k_j}\},\quad \forall i\)</span>.</p>
</div>
<div id="powells-example" class="section level1">
<h1>Powell's example</h1>
<p>In <em>ON SEARCH DIRECTIONS FOR MINIMIZATION ALGORITHMS</em>, Power actually gives three examples that sequences generated by the algorithm discussed above do not convergence to stationary points once some hypothesis are not met.</p>
<ul>
<li><p>The first example is straightforward, However, the remarkable properties of this example can be destroyed by making a small perturbation to the starting vector <span class="math inline">\(x^0\)</span>.</p></li>
<li><p>The second example is not sensitive to either small changes in the initial data or to small errors introduced during the iterative process, for example computer rounding errors.</p></li>
<li><p>The third example suggests that a function that is infinitely differentiable that also causes an endless loop in the iterative minimization method.</p></li>
</ul>
<p>We here only presents the first example. Consider the following function</p>
<p><span class="math display">\[f(x,y,z)=-(xy+yz+zx)+(x-1)_+^2+(-x-1)_+^2+(y-1)_+^2+(-y-1)_+^2+(z-1)_+^2+(-z-1)_+^2\]</span></p>
<p>where</p>
<p><span class="math display">\[(x-c)_+^2=\begin{cases}0,x-c&lt; 0\\ (x-c)^2,x-c\geq 0\end{cases}\]</span></p>
<p>Given the starting point <span class="math inline">\(x_0=(-1-e,1+\frac{1}{2}e,-1-\frac{1}{4}e)\)</span> and use block coordinate decent algorithm,and we update the variable in a manner of <span class="math inline">\(x\rightarrow y\rightarrow z\rightarrow x ...\)</span> with</p>
<p><span class="math display">\[x_{k+1}^{**}\leftarrow \text{sign}(y_k+z_k)[1+\frac{1}{2}\vert y_k+z_k\vert ]\]</span></p>
<p><span class="math display">\[y_{k+1}^{**}\leftarrow \text{sign}(x_{k+1}+z_k)[1+\frac{1}{2}\vert x_{k+1}+z_k\vert ]\]</span></p>
<p><span class="math display">\[z_{k+1}^{**}\leftarrow \text{sign}(x_{k+1}+y_{k+1})[1+\frac{1}{2}\vert x_{k+1}+y_{k+1}\vert ]\]</span></p>
<p>We here present the first six steps of this case</p>
<table>
<thead>
<tr class="header">
<th>cycle/totall iteration</th>
<th>x</th>
<th>y</th>
<th>z</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1/1</td>
<td>1+<span class="math inline">\(\frac{1}{8}e\)</span></td>
<td>1+$e $</td>
<td>-1-<span class="math inline">\(\frac{1}{4}e\)</span></td>
</tr>
<tr class="even">
<td>1/2</td>
<td>1+<span class="math inline">\(\frac{1}{8}e\)</span></td>
<td>-1-<span class="math inline">\(\frac{1}{16}e\)</span></td>
<td>-1-<span class="math inline">\(\frac{1}{4}e\)</span></td>
</tr>
<tr class="odd">
<td>1/3</td>
<td>1+<span class="math inline">\(\frac{1}{8}e\)</span></td>
<td>-1-<span class="math inline">\(\frac{1}{16}e\)</span></td>
<td>1+<span class="math inline">\(\frac{1}{32}e\)</span></td>
</tr>
<tr class="even">
<td>2/4</td>
<td>-1-<span class="math inline">\(\frac{1}{64}e\)</span></td>
<td>-1-<span class="math inline">\(\frac{1}{16}e\)</span></td>
<td>1+<span class="math inline">\(\frac{1}{32}e\)</span></td>
</tr>
<tr class="odd">
<td>2/5</td>
<td>-1-<span class="math inline">\(\frac{1}{64}e\)</span></td>
<td>1+<span class="math inline">\(\frac{1}{128}e\)</span></td>
<td>1+<span class="math inline">\(\frac{1}{32}e\)</span></td>
</tr>
<tr class="even">
<td>2/6</td>
<td>-1-<span class="math inline">\(\frac{1}{64}e\)</span></td>
<td>1+<span class="math inline">\(\frac{1}{128}e\)</span></td>
<td>-1-<span class="math inline">\(\frac{1}{256}e\)</span></td>
</tr>
<tr class="odd">
<td>3/7</td>
<td>1+<span class="math inline">\(\frac{1}{512}e\)</span></td>
<td>1+<span class="math inline">\(\frac{1}{128}e\)</span></td>
<td>-1-<span class="math inline">\(\frac{1}{256}e\)</span></td>
</tr>
<tr class="even">
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
</tbody>
</table>
<p>This result implies that the sequence obtained by this algorithm can not converge to one single point since <span class="math inline">\(x-coordinate\)</span> change its sign as the even cycle and odd cycle alternate. Situations are similar for <span class="math inline">\(y-coordinate\)</span> and <span class="math inline">\(z-coordinate\)</span>.</p>
<p>But <span class="math inline">\(\{x_k\}\)</span> has six sub-sequences which convergence to (1,1,-1), (1,-1,-1), (1,-1,1), (-1,-1,1),(-1,-1,1),(-1,1,1),(-1,1,-1) respectively.</p>
<div class="figure">
<img src="14748787199151.jpg" />

</div>
<p><strong>Remark</strong></p>
<ol style="list-style-type: decimal">
<li><p>A hint to derive the update formula:</p>
<p><span class="math display">\[x\leftarrow \text{sign}(y+z)[1+\frac{1}{2}(y+z)]\]</span></p>
<p>Indeed, derivates of <span class="math inline">\((x-1)_+^2\)</span> and <span class="math inline">\((-x-1)_+^2\)</span> are as follows respecively</p>
<p><span class="math display">\[\frac{d(x-1)_+^2}{dx}=\begin{cases}2(x-1),x\geq 1\\0,x&lt;1\end{cases}\quad 
    \frac{d(-x-1)_+^2}{dx}=\begin{cases}2(-x-1),x\leq -1\\0,x&gt;-1\end{cases}
\]</span></p>
<p>So for the univariate optimization problem, setting the derivate of <span class="math inline">\(g(x)=f(x,y,z)\)</span> to zero, we conclude</p>
<p><span class="math display">\[\frac{\partial f(x,y,x)}{\partial x}=0\Rightarrow 
\begin{cases}x\geq 1: x=1+\frac{1}{2}(y+z)\\-1&lt; x&lt;1: -(y+z)=0\\x\leq -1:x=-1+\frac{1}{2}(y+z) \end{cases}\]</span></p></li>
<li><p>The gradient of <span class="math inline">\(f(x,y,z)\)</span> on this cyclic path, is <span class="math inline">\(\nabla f(x,y,z)=(-y-z,-x-z,-x-y)\)</span> and <span class="math inline">\(\vert \vert \nabla f(x,y,z)\vert \vert _1=2\)</span></p></li>
<li><p>This example is unstable with respect to small perturbations. Small changes in the starting point <span class="math inline">\(x_0=(-1-e,1+\frac{1}{2}e,-1-\frac{1}{4}e)\)</span> or smal errors in the numbers that are computed during the calculation will destroy the cyclic behavior.</p>
<p>It's s clear the choice of perturbations <span class="math inline">\(e\)</span> plays a key role. Say, <span class="math inline">\(x_0=(-1-e_1,1+e_2,-1-e_3)\)</span> and we have <span class="math inline">\(e_k=\frac{1}{2}(e_{k-2}- e_{k-1})\)</span></p>
<table>
<thead>
<tr class="header">
<th>cycle/totall iteration</th>
<th>x</th>
<th>y</th>
<th>z</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1/1</td>
<td>1+<span class="math inline">\(e_4\)</span></td>
<td>1+<span class="math inline">\(e_2\)</span></td>
<td>-1-<span class="math inline">\(e_3\)</span></td>
</tr>
<tr class="even">
<td>1/2</td>
<td>1+<span class="math inline">\(e_4\)</span></td>
<td>-1-<span class="math inline">\(e_5\)</span></td>
<td>-1-<span class="math inline">\(e_3\)</span></td>
</tr>
<tr class="odd">
<td>1/3</td>
<td>1+<span class="math inline">\(e_4\)</span></td>
<td>-1-<span class="math inline">\(e_5\)</span></td>
<td>1+<span class="math inline">\(e_6\)</span></td>
</tr>
<tr class="even">
<td>2/4</td>
<td>-1-<span class="math inline">\(e_7\)</span></td>
<td>-1-<span class="math inline">\(e_5\)</span></td>
<td>1+<span class="math inline">\(e_6\)</span></td>
</tr>
<tr class="odd">
<td>2/5</td>
<td>-1-<span class="math inline">\(e_7\)</span></td>
<td>1+<span class="math inline">\(e_8\)</span></td>
<td>1+<span class="math inline">\(e_6\)</span></td>
</tr>
<tr class="even">
<td>2/6</td>
<td>-1-<span class="math inline">\(e_7\)</span></td>
<td>1+<span class="math inline">\(e_8\)</span></td>
<td>-1-<span class="math inline">\(e_9\)</span></td>
</tr>
<tr class="odd">
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
</tbody>
</table>
<p>To preserve the cyclic behavior , we have to make sure that <span class="math inline">\(e_{k-2}&gt;e_{k-1}\)</span></p>
<p>And in practice, when we do some numerical tests, we shall find that, this theoretically-existed endless loop actual breaks down due to the rounding errors. A brief illustration is given below. In this experiment, loop ends at the 52 steps.</p>
<p><img src="Snip20161117_13.png" alt="Snip20161117_13" /><br />
<img src="Snip20161117_14.png" alt="Snip20161117_14" /> <img src="Snip20161117_15.png" alt="Snip20161117_15" /></p></li>
<li><p>As<br />
<span class="math display">\[\frac{\partial f(x,y,x)}{\partial x}=0\Rightarrow 
\begin{cases}x\geq 1: x=1+\frac{1}{2}(y+z)\\-1&lt; x&lt;1: -(y+z)=0\\x\leq -1:x=-1+\frac{1}{2}(y+z) \end{cases}\]</span></p>
<p>suggests that, when <span class="math inline">\(-1&lt;x&lt;1\)</span>, the choice of <span class="math inline">\(x\)</span> is arbitrary and we set <span class="math inline">\(x^*=0\)</span> in the case above. So the uniqueness requirement is violated. It turns out that the six vertices are even not the stationary points.</p>
<p>For example, at point <span class="math inline">\(\bar x=(1,1,-1)\)</span>, <span class="math inline">\(\nabla f(\bar x)=(0,0,-2)\)</span> and for any ponit <span class="math inline">\(x\)</span> in the unit cubic <span class="math inline">\((x-\bar x)^T\nabla f(\bar x)\leq 0\)</span>. Say, <span class="math inline">\(x=(0.9,0.9,-0.9)\)</span>, <span class="math inline">\((x-\bar x)^T\nabla f(\bar x)=-0.2&lt;0\)</span></p>
<p>Actually, as in the proof of <code>Theorem</code>, we prove that <span class="math inline">\(\{z_1^{k_j}\}\)</span> converges to <span class="math inline">\(\bar x\)</span>, where <span class="math inline">\(\bar x\)</span> is the limit point of <span class="math inline">\(\{x^{k_j}\}\)</span>. But in this example, the limit point of <span class="math inline">\(\{z_1^{k_j}\}\)</span> is (1,1,-1) while the limit point of <span class="math inline">\(\{x^{k_j}\}\)</span> is either (-1,1,-1) or (1,-1,1). So the requirement of uniqueness is not met.</p></li>
</ol>
</div>
<div id="r-codes-for-numerical-experiments" class="section level1">
<h1>R codes for numerical experiments</h1>
<pre class="r"><code>####################
### Function for test ###
####################

PowellE1&lt;-function(xstart,cycles,fig=T){
  #######function part ##############
  UpdateCycle&lt;-function(x){
    Sign&lt;-function(x){
      if (x&gt;0){
        return(1)
      }else{
        if (x&lt;0){
          return(-1)
        }else{
          return(0)
        }
      }
    }
    x.new&lt;-c()
    x.new[1]&lt;-Sign(x[2]+x[3])*(1+0.5*abs(x[2]+x[3]))
    x.new[2]&lt;-Sign(x.new[1]+x[3])*(1+0.5*abs(x.new[1]+x[3]))
    x.new[3]&lt;-Sign(x.new[1]+x.new[2])*(1+0.5*abs(x.new[1]+x.new[2]))
    cycle&lt;-matrix(c(x.new[1],x[2],x[3],x.new[1],x.new[2],x[3],x.new[1],x.new[2],x.new[3]),
                  ncol=3,byrow=T)
    return(cycle)
  }
  
  fpowell&lt;-function(x){
    
    PostivePart&lt;-function(x){
      ifelse(x&gt;=0,x,0)
    }
    
    fval&lt;-(-(x[1]*x[2]+x[2]*x[3]+x[1]*x[3]))+
      PostivePart(x[1]-1)^2+PostivePart(-x[1]-1)^2+
      PostivePart(x[2]-1)^2+PostivePart(-x[2]-1)^2+
      PostivePart(x[3]-1)^2+PostivePart(-x[3]-1)^2
    return(fval)
  }
  ############ operation part ################
  x.store&lt;-matrix(ncol=3,nrow=cycles*3+1)
  x.store[1,]&lt;-xstart
  for (i in seq_len(cycles)){
    x.store[(3*i-1):(3*i+1),]&lt;-UpdateCycle(x.store[3*i-2,])
  }
  x.store&lt;-x.store[-1,]
  fval&lt;-rep(0,cycles*3)
  
  for(i in seq_len(cycles*3)){
    fval[i]&lt;-fpowell(x.store[i,])
  }
  fval&lt;-as.matrix(fval)
  
  if (fig==T){
    plot(fval,ylim=c(min(fval)-1,max(fval)+1),type=&quot;l&quot;,xlab=&quot;Iterations&quot;,ylab = &quot;F value&quot;)
  }
  r&lt;-list()
  r$x.iterate&lt;-x.store
  r$fval&lt;-fval
  return(r)
}


##################
#### Test 1 ########
##################


perturb&lt;-0.5
xstart&lt;-c(-1-perturb,1+0.5*perturb,-1-0.25*perturb)
cycles&lt;-20

r&lt;-PowellE1(xstart,cycles,fig=T)

##################
#### Test 2 ########
##################

perturb&lt;-0.5
xstart&lt;-c(-1-perturb,1+0.5*perturb,-1-0.25*perturb)
cycles&lt;-20

r&lt;-PowellE1(xstart,cycles,fig=T)</code></pre>
<p><img src="/post/powell/index_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<pre class="r"><code>##################
#### Test 3 ########
##################

xstart&lt;-c(3,2,1)
cycles&lt;-100

r&lt;-PowellE1(xstart,cycles,fig=T)</code></pre>
<p><img src="/post/powell/index_files/figure-html/unnamed-chunk-1-2.png" width="672" /></p>
</div>
